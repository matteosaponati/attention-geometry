\begin{table}
    \label{table:symmetry-score-models}
    \caption{Symmetry score for open source pretrained language models. All models are available on Huggingface \citep{wolfHuggingFaceTransformersStateoftheart2020}.}
    \vspace{5pt}
    \centering
    \begin{tabular}{lcc|lcc}
        \toprule
        \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} & \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} \\ 
        \midrule
    bert-tiny & 0.77 & $\pm$ [0.07, 0.07] & gpt-neo-1.3b & 0.14 & $\pm$ [0.03, 0.03] \\ 
bert-mini & 0.62 & $\pm$ [0.03, 0.05] & gpt-neo-2.7b & 0.13 & $\pm$ [0.02, 0.04] \\ 
bert-small & 0.69 & $\pm$ [0.1, 0.08] & gpt-j-6b & 0.11 & $\pm$ [0.02, 0.03] \\ 
bert-medium & 0.6 & $\pm$ [0.01, 0.02] & openai-gpt & 0.07 & $\pm$ [0.04, 0.03] \\ 
bert-base & 0.51 & $\pm$ [0.09, 0.07] & gpt2-xl & 0.12 & $\pm$ [0.03, 0.05] \\ 
bert-large & 0.44 & $\pm$ [0.03, 0.08] & istilbert/distilgpt2 & 0.19 & $\pm$ [0.05, 0.05] \\ 
bert-distill & 0.43 & $\pm$ [0.1, 0.13] & utherAI/gpt-neo-125m & 0.14 & $\pm$ [0.09, 0.14] \\ 
ncased_L-2_H-128_A-2 & 0.77 & $\pm$ [0.07, 0.07] & utherAI/gpt-neo-1.3B & 0.14 & $\pm$ [0.03, 0.03] \\ 
ncased_L-4_H-256_A-4 & 0.62 & $\pm$ [0.03, 0.05] & utherAI/gpt-neo-2.7B & 0.14 & $\pm$ [0.03, 0.02] \\ 
ncased_L-4_H-512_A-8 & 0.69 & $\pm$ [0.1, 0.08] & EleutherAI/gpt-j-6B & 0.11 & $\pm$ [0.02, 0.03] \\ 
ncased_L-8_H-512_A-8 & 0.6 & $\pm$ [0.01, 0.02] & llama2-7b & 0.12 & $\pm$ [0.02, 0.03] \\ 
bert-base-uncased & 0.51 & $\pm$ [0.09, 0.07] & llama2-13b & 0.17 & $\pm$ [0.02, 0.02] \\ 
bert-large-uncased & 0.44 & $\pm$ [0.03, 0.08] & llama3-8b & 0.0 & $\pm$ [0.0, 0.01] \\ 
tilbert-base-uncased & 0.43 & $\pm$ [0.1, 0.13] & llama3.1-8b & 0.0 & $\pm$ [0.0, 0.01] \\ 
se-patch16-224-pt22k & 0.4 & $\pm$ [0.08, 0.02] & llama3.2-8b & 0.01 & $\pm$ [0.01, 0.01] \\ 
ge-patch16-224-pt22k & 0.33 & $\pm$ [0.05, 0.07] & llama3.2-1b & 0.01 & $\pm$ [0.01, 0.01] \\ 
eit-base-patch16-224 & 0.39 & $\pm$ [0.23, 0.07] & llama3.2-3b & 0.01 & $\pm$ [0.01, 0.01] \\ 
it-large-patch16-224 & 0.26 & $\pm$ [0.17, 0.13] & -llama/Llama-2-7b-hf & 0.12 & $\pm$ [0.02, 0.03] \\ 
eit-base-patch16-384 & 0.39 & $\pm$ [0.23, 0.07] & llama/Llama-2-70b-hf & 0.02 & $\pm$ [0.01, 0.02] \\ 
it-large-patch16-384 & 0.26 & $\pm$ [0.17, 0.13] & a/Llama-2-7b-chat-hf & 0.12 & $\pm$ [0.02, 0.03] \\ 
it-large-patch16-512 & 0.26 & $\pm$ [0.17, 0.13] & /Llama-2-13b-chat-hf & 0.17 & $\pm$ [0.02, 0.02] \\ 
albert-base-v2 & 0.72 & $\pm$ [0.0, 0.0] & lama/Meta-Llama-3-8B & 0.0 & $\pm$ [0.0, 0.0] \\ 
albert-large-v2 & 0.7 & $\pm$ [0.0, 0.0] & ama/Meta-Llama-3-70B & 0.02 & $\pm$ [0.01, 0.01] \\ 
albert-xlarge-v2 & 0.59 & $\pm$ [0.0, 0.0] & a-llama/Llama-3.1-8B & 0.0 & $\pm$ [0.0, 0.01] \\ 
albert-xxlarge-v2 & 0.46 & $\pm$ [0.0, 0.0] & -llama/Llama-3.1-70B & 0.01 & $\pm$ [0.0, 0.01] \\ 
ebookAI/roberta-base & 0.49 & $\pm$ [0.03, 0.06] & llama/Llama-3.1-405B & 0.03 & $\pm$ [0.01, 0.03] \\ 
bookAI/roberta-large & 0.47 & $\pm$ [0.06, 0.06] & a-llama/Llama-3.2-1B & 0.0 & $\pm$ [0.0, 0.0] \\ 
kAI/xlm-roberta-base & 0.51 & $\pm$ [0.05, 0.03] & a-llama/Llama-3.2-3B & 0.01 & $\pm$ [0.01, 0.01] \\ 
AI/xlm-roberta-large & 0.49 & $\pm$ [0.16, 0.12] & alai/Mistral-7B-v0.1 & 0.0 & $\pm$ [0.0, 0.01] \\ 
I/roberta-large-mnli & 0.47 & $\pm$ [0.06, 0.06] & i/Mixtral-8x22B-v0.1 & 0.0 & $\pm$ [0.0, 0.0] \\ 
t/distilroberta-base & 0.53 & $\pm$ [0.02, 0.06] & ebook/MobileLLM-125M & 0.03 & $\pm$ [0.02, 0.03] \\ 
otai/ModernBERT-base & 0.18 & $\pm$ [0.06, 0.18] & ebook/MobileLLM-350M & 0.01 & $\pm$ [0.01, 0.01] \\ 
tai/ModernBERT-large & 0.2 & $\pm$ [0.06, 0.16] & ebook/MobileLLM-600M & 0.01 & $\pm$ [0.01, 0.01] \\ 
gpt1 & 0.07 & $\pm$ [0.04, 0.03] & acebook/MobileLLM-1B & 0.01 & $\pm$ [0.01, 0.01] \\ 
gpt2 & 0.15 & $\pm$ [0.02, 0.03] & ebook/MobileLLM-1.5B & 0.01 & $\pm$ [0.01, 0.01] \\ 
gpt2-medium & 0.17 & $\pm$ [0.03, 0.05] & microsoft/phi-1_5 & 0.09 & $\pm$ [0.03, 0.03] \\ 
gpt2-large & 0.17 & $\pm$ [0.04, 0.02] & microsoft/phi-1 & 0.14 & $\pm$ [0.02, 0.01] \\ 
gpt2-xlarge & 0.12 & $\pm$ [0.03, 0.05] & microsoft/phi-2 & 0.07 & $\pm$ [0.03, 0.06] \\ 
gpt2-distill & 0.19 & $\pm$ [0.05, 0.05] & i-3-mini-4k-instruct & 0.0 & $\pm$ [0.0, 0.0] \\ 
gpt-neo-125m & 0.14 & $\pm$ [0.09, 0.14] & 3-mini-128k-instruct & 0.12 & $\pm$ [0.04, 0.03] \\ 
    \bottomrule
    \end{tabular}
    \end{table}