\begin{table}
    \label{table:symmetry-score-models}
    \caption{Symmetry score for open source pretrained language models. All models are available on Huggingface \citep{wolfHuggingFaceTransformersStateoftheart2020}.}
    \vspace{5pt}
    \centering
    \begin{tabular}{lcc|lcc}
        \toprule
        \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} & \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} \\ 
        \midrule
    bert-tiny & -0.79 & $\pm$ [0.11, 0.11] & gpt-neo-1.3b & -0.49 & $\pm$ [0.19, 0.13] \\ 
bert-mini & -0.33 & $\pm$ [0.03, 0.04] & gpt-neo-2.7b & -0.57 & $\pm$ [0.15, 0.16] \\ 
bert-small & -0.22 & $\pm$ [0.04, 0.03] & gpt-j-6b & -0.28 & $\pm$ [0.09, 0.08] \\ 
bert-medium & -0.23 & $\pm$ [0.06, 0.1] & openai-gpt & -0.18 & $\pm$ [0.08, 0.07] \\ 
bert-base & -0.08 & $\pm$ [0.02, 0.03] & gpt2-xl & -0.23 & $\pm$ [0.11, 0.1] \\ 
bert-large & -0.03 & $\pm$ [0.02, 0.06] & distilbert/distilgpt2 & -0.51 & $\pm$ [0.03, 0.07] \\ 
bert-distill & -0.13 & $\pm$ [0.0, 0.06] & EleutherAI/gpt-neo-125m & -0.56 & $\pm$ [0.21, 0.08] \\ 
google/bert_uncased_L-2_H-128_A-2 & -0.79 & $\pm$ [0.11, 0.11] & EleutherAI/gpt-neo-1.3B & -0.49 & $\pm$ [0.19, 0.13] \\ 
google/bert_uncased_L-4_H-256_A-4 & -0.33 & $\pm$ [0.03, 0.04] & EleutherAI/gpt-neo-2.7B & -0.49 & $\pm$ [0.15, 0.21] \\ 
google/bert_uncased_L-4_H-512_A-8 & -0.22 & $\pm$ [0.04, 0.03] & EleutherAI/gpt-j-6B & -0.28 & $\pm$ [0.09, 0.08] \\ 
google/bert_uncased_L-8_H-512_A-8 & -0.23 & $\pm$ [0.06, 0.1] & llama2-7b & -0.26 & $\pm$ [0.09, 0.13] \\ 
bert-base-uncased & -0.08 & $\pm$ [0.02, 0.03] & llama2-13b & -0.15 & $\pm$ [0.11, 0.03] \\ 
bert-large-uncased & -0.03 & $\pm$ [0.02, 0.06] & llama3-8b & -0.65 & $\pm$ [0.13, 0.2] \\ 
distilbert-base-uncased & -0.13 & $\pm$ [0.0, 0.06] & llama3.1-8b & -0.64 & $\pm$ [0.17, 0.19] \\ 
microsoft/beit-base-patch16-224-pt22k & -0.1 & $\pm$ [0.06, 0.15] & llama3.2-8b & -0.59 & $\pm$ [0.18, 0.22] \\ 
microsoft/beit-large-patch16-224-pt22k & -0.15 & $\pm$ [0.08, 0.07] & llama3.2-1b & -0.59 & $\pm$ [0.18, 0.22] \\ 
microsoft/beit-base-patch16-224 & -0.14 & $\pm$ [0.15, 0.21] & llama3.2-3b & -0.77 & $\pm$ [0.08, 0.19] \\ 
microsoft/beit-large-patch16-224 & -0.14 & $\pm$ [0.04, 0.14] & meta-llama/Llama-2-7b-hf & -0.29 & $\pm$ [0.07, 0.14] \\ 
microsoft/beit-base-patch16-384 & -0.14 & $\pm$ [0.15, 0.21] & meta-llama/Llama-2-70b-hf & -0.24 & $\pm$ [0.1, 0.06] \\ 
microsoft/beit-large-patch16-384 & -0.14 & $\pm$ [0.04, 0.14] & meta-llama/Llama-2-7b-chat-hf & -0.29 & $\pm$ [0.07, 0.14] \\ 
microsoft/beit-large-patch16-512 & -0.15 & $\pm$ [0.03, 0.14] & meta-llama/Llama-2-13b-chat-hf & -0.19 & $\pm$ [0.12, 0.04] \\ 
albert-base-v2 & -0.07 & $\pm$ [0.0, 0.0] & meta-llama/Meta-Llama-3-8B & 0.01 & $\pm$ [0.05, 0.05] \\ 
albert-large-v2 & -0.17 & $\pm$ [0.0, 0.0] & meta-llama/Meta-Llama-3-70B & -0.37 & $\pm$ [0.09, 0.12] \\ 
albert-xlarge-v2 & -0.24 & $\pm$ [0.0, 0.0] & meta-llama/Llama-3.1-8B & -0.57 & $\pm$ [0.16, 0.13] \\ 
albert-xxlarge-v2 & -0.15 & $\pm$ [0.0, 0.0] & meta-llama/Llama-3.1-70B & -0.37 & $\pm$ [0.08, 0.11] \\ 
FacebookAI/roberta-base & -0.12 & $\pm$ [0.11, 0.03] & meta-llama/Llama-3.1-405B & -0.17 & $\pm$ [0.07, 0.07] \\ 
FacebookAI/roberta-large & -0.06 & $\pm$ [0.03, 0.03] & meta-llama/Llama-3.2-1B & -0.02 & $\pm$ [0.13, 0.08] \\ 
FacebookAI/xlm-roberta-base & -0.02 & $\pm$ [0.02, 0.02] & meta-llama/Llama-3.2-3B & -0.7 & $\pm$ [0.07, 0.22] \\ 
FacebookAI/xlm-roberta-large & -0.02 & $\pm$ [0.03, 0.02] & mistralai/Mistral-7B-v0.1 & -0.58 & $\pm$ [0.15, 0.13] \\ 
FacebookAI/roberta-large-mnli & -0.06 & $\pm$ [0.03, 0.03] & mistralai/Mixtral-8x22B-v0.1 & -0.66 & $\pm$ [0.09, 0.16] \\ 
distilbert/distilroberta-base & -0.14 & $\pm$ [0.09, 0.07] & facebook/MobileLLM-125M & -0.13 & $\pm$ [0.15, 0.1] \\ 
answerdotai/ModernBERT-base & -0.04 & $\pm$ [0.05, 0.04] & facebook/MobileLLM-350M & -0.34 & $\pm$ [0.13, 0.23] \\ 
answerdotai/ModernBERT-large & 0.01 & $\pm$ [0.08, 0.02] & facebook/MobileLLM-600M & -0.39 & $\pm$ [0.1, 0.26] \\ 
gpt1 & -0.18 & $\pm$ [0.08, 0.07] & facebook/MobileLLM-1B & -0.4 & $\pm$ [0.13, 0.11] \\ 
gpt2 & -0.58 & $\pm$ [0.06, 0.14] & facebook/MobileLLM-1.5B & -0.47 & $\pm$ [0.1, 0.08] \\ 
gpt2-medium & -0.42 & $\pm$ [0.09, 0.08] & microsoft/phi-1_5 & -0.28 & $\pm$ [0.22, 0.19] \\ 
gpt2-large & -0.32 & $\pm$ [0.06, 0.11] & microsoft/phi-1 & -0.4 & $\pm$ [0.03, 0.04] \\ 
gpt2-xlarge & -0.23 & $\pm$ [0.11, 0.1] & microsoft/phi-2 & -0.24 & $\pm$ [0.25, 0.34] \\ 
gpt2-distill & -0.51 & $\pm$ [0.03, 0.07] & microsoft/Phi-3-mini-4k-instruct & -0.0 & $\pm$ [0.05, 0.06] \\ 
gpt-neo-125m & -0.56 & $\pm$ [0.21, 0.08] & microsoft/Phi-3-mini-128k-instruct & 0.07 & $\pm$ [0.04, 0.02] \\ 
    \bottomrule
    \end{tabular}
    \end{table}