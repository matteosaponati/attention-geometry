{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM\n",
    "from transformers import BertModel, AlbertModel, DistilBertModel, RobertaModel, OpenAIGPTModel, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(A):\n",
    "    \"\"\"\n",
    "    Given a square matrix A, calculate the symmetric (S) and skew-symmetric (N) scores of a matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        square numpy matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple :\n",
    "        Symmetric (S) and skew-symmetric (N) scores.\n",
    "    \"\"\"\n",
    "     \n",
    "    S = np.linalg.norm(.5 * (A + A.T), 'fro') / np.linalg.norm(A, 'fro')\n",
    "    N = np.linalg.norm(.5 * (A - A.T), 'fro') / np.linalg.norm(A, 'fro')\n",
    "\n",
    "    return S, N\n",
    "\n",
    "def scores_layer(d,h,dh,Wq,Wk):\n",
    "\n",
    "    Sheads = np.zeros(h)\n",
    "    Nheads = np.zeros(h)\n",
    "\n",
    "    for j, head in enumerate(range(0, d, dh)):\n",
    "        M = Wq[:, head: head + dh] @ Wk[head : head + dh,:]\n",
    "        Sheads[j], Nheads[j] = scores(M)   \n",
    "\n",
    "    return  Sheads, Nheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Let Q and K be\n",
    "Q = X @ W_q ; K = X @ W_k\n",
    "\n",
    "it follows that the dot product between queries and keys is \n",
    "Q @ K^T = X @ (W_q @ W_k^T) @ X^T = X @ M @ X^T \n",
    "\n",
    "where M is a square matrix \\in R^{d,d}, that can be decomposed into its\n",
    "symmetric and skew-symmetric part S and N, respectively,\n",
    "M = 1/2 * (M + M^T) + 1/2 * (M - M^T) = S + N\n",
    "\n",
    "important: nn.Linear.weight returns the learnable weights of the module \n",
    "of shape (out_features,in_features). Thereby, we get the matrix W_q or \n",
    "W_k as nn.Linear.weight^T, and thus M = Wq.T @ Wk in this case.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def getscoresBERT(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.encoder.layer[i].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.layer[i].attention.self.key.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresDistillBERT(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.layer[i].attention.q_lin.weight.detach().numpy()\n",
    "        Wk = model.transformer.layer[i].attention.k_lin.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresALBERT(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.encoder.albert_layer_groups[i].albert_layers[0].attention.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.albert_layer_groups[i].albert_layers[0].attention.key.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresDistillROBERTA(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.roberta.encoder.layer[i].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.roberta.encoder.layer[i].attention.self.key.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPT(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.h[i].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.h[i].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq,Wk.T)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPTneo(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.h[i].attn.attention.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[i].attn.attention.k_proj.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPTneox(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.gpt_neox.layers[i].attention.query_key_value.weight[:d,:].detach().numpy()\n",
    "        Wk = model.gpt_neox.layers[i].attention.query_key_value.weight[d:2*d,:].detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPTj(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.h[i].attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[i].attn.k_proj.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresDistillGPT(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.h[i].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.transformer.h[i].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresOPT(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.model.decoder.layers[i].self_attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.model.decoder.layers[i].self_attn.k_proj.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresT5(d,l,h,dh,model):\n",
    "\n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "    for i in range(l):\n",
    "        Wq = model.encoder.block[i].layer[0].SelfAttention.q.weight.detach().numpy()\n",
    "        Wk = model.encoder.block[i].layer[0].SelfAttention.k.weight.detach().numpy()\n",
    "        S[i, :], N[i, :] = scores_layer(d,h,dh,Wq.T,Wk)\n",
    "\n",
    "    return  S, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- KEY (str): model name\n",
    "- VALUES (list): [layers (int), embedding dim (int), heads (int), head dim (int), S scores, N scores]\n",
    "\"\"\"\n",
    "\n",
    "if os.path.isfile('../data/fig_scores/models.pkl'):\n",
    "    with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "        models = pickle.load(file)\n",
    "else: models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "BERT models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "dh = 64\n",
    "\n",
    "'BERT tiny (l = 2, d = 128, h = 2 ; 4.40M parameters)'\n",
    "l = 2\n",
    "d = 128\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTtiny'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT mini (l = 4, d = 256, h = 4 ; 11.3M parameters)'\n",
    "l = 4\n",
    "d = 256\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTmini'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT small (l = 4, d = 512, h = 8 ; 29.1M parameters)'\n",
    "l = 4\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-512_A-8\")\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTsmall'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT medium (l = 8, d = 512, h = 8 ; 41.7M parameters)'\n",
    "l = 8\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-8_H-512_A-8\")\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTmedium'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT base (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT large (masking) (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTlarge_mask'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillBERT base model (l = 6, d = 768, h = 12 ; tot num parameters 66M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "S, N = getscoresDistillBERT(d,l,h,dh,model)\n",
    "models['DistillBERT'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "ROBERTA models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'ROBERTA base (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['ROBERTAbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ROBERTA large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "S, N = getscoresBERT(d,l,h,dh,model)\n",
    "models['ROBERTAlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillROBERTA base (l = 6, d = 768, h = 12 ; tot num parameters 82M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "S, N = getscoresDistillROBERTA(d,l,h,dh,model)\n",
    "models['DistillROBERTA'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "ALBERT models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "    \n",
    "dh = 64\n",
    "\n",
    "'ALBERT base model (l = 12, d = 768, h = 12 ; tot num parameters 11M)'\n",
    "l = 1\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "S, N = getscoresALBERT(d,l,h,dh,model)\n",
    "models['ALBERTbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT large model (l = 24, d = 1024, h = 16 ; tot num parameters 17M)'\n",
    "l = 1\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-large-v2\")\n",
    "S, N = getscoresALBERT(d,l,h,dh,model)\n",
    "models['ALBERTlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT xlarge model (l = 24, d = 2048, h = 16 ; tot num parameters 58M)'\n",
    "dh = 64\n",
    "l = 1\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
    "S, N = getscoresALBERT(d,l,h,dh,model)\n",
    "models['ALBERTxlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT xxlarge model (l = 12, d = 4096, h = 64 ; tot num parameters 223M)'\n",
    "dh = 64\n",
    "l = 1\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-xxlarge-v2\")\n",
    "S, N = getscoresALBERT(d,l,h,dh,model)\n",
    "models['ALBERTxxlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generative Pre-trained Transformers (GPT) models \n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'GPT 1 (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "S, N = getscoresGPT(d,l,h,dh,model)\n",
    "models['GPT'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 (l = 12, d = 768, h = 12 ; 117M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "S, N = getscoresGPT(d,l,h,dh,model)\n",
    "models['GPT2'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 medium (l = 24, d = 1024, h = 16 ; 345M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "S, N = getscoresGPT(d,l,h,dh,model)\n",
    "models['GPT2medium'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 large (l = 36, d = 1280, h = 20 ; 774M parameters)'\n",
    "l = 36\n",
    "d = 1280\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-large')\n",
    "S, N = getscoresGPT(d,l,h,dh,model)\n",
    "models['GPT2large'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 xl (l = 48, d = 1600, h = 25 ; 1558M parameters)'\n",
    "l = 48\n",
    "d = 1600\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "S, N = getscoresGPT(d,l,h,dh,model)\n",
    "models['GPT2xl'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillGPT2 base model (l = 12, d = 768, h = 12 ; tot num parameters 82M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "S, N = getscoresDistillGPT(d,l,h,dh,model)\n",
    "models['DistillGPT2'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "GPT Neo models (EleutherAI)\n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "'GPT neo 125m (l = 12, d = 768, h = 12)'\n",
    "dh = 64\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "S, N = getscoresGPTneo(d,l,h,dh,model)\n",
    "models['GPTneo-125m'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT neo 1.3b (l = 12, d = 768, h = 16)'\n",
    "dh = 128\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "S, N = getscoresGPTneo(d,l,h,dh,model)\n",
    "models['GPTneo-1.3b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT neo 2.7b (l = 12, d = 768, h = 20)'\n",
    "dh = 128\n",
    "l = 32\n",
    "d = 2560\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "S, N = getscoresGPTneo(d,l,h,dh,model)\n",
    "models['GPTneo-2.7b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "# 'GPT neox 20b (l = 44, d = 768, h = 64)'\n",
    "# dh = 96\n",
    "# l = 44\n",
    "# d = 6144\n",
    "# h = d // dh\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "# S, N = getscoresGPTneox(d,l,h,dh,model)\n",
    "# models['GPTneox-20b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "# with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "#     pickle.dump(models, file)\n",
    "\n",
    "'GPT-j 6b (l = 28, d = 4096, h = 16)'\n",
    "dh = 256\n",
    "l = 28\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
    "S, N = getscoresGPTj(d,l,h,dh,model)\n",
    "models['GPTj-6b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m\n\u001b[1;32m     84\u001b[0m h \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m dh\n\u001b[0;32m---> 85\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/opt-30b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m S, N \u001b[38;5;241m=\u001b[39m getscoresOPT(d,l,h,dh,model)\n\u001b[1;32m     87\u001b[0m models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPT-30b\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [l,d,h,dh,S,N]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/modeling_utils.py:2876\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2876\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:823\u001b[0m, in \u001b[0;36mOPTForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mOPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;66;03m# the lm_head weight is automatically tied to the embed tokens weight\u001b[39;00m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mword_embed_proj_dim, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:755\u001b[0m, in \u001b[0;36mOPTModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: OPTConfig):\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mOPTDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:520\u001b[0m, in \u001b[0;36mOPTDecoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[43m[\u001b[49m\u001b[43mOPTDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:520\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mOPTDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:294\u001b[0m, in \u001b[0;36mOPTDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mactivation_function]\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, elementwise_affine\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_elementwise_affine\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mffn_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_bias)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, elementwise_affine\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_elementwise_affine)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/torch/nn/init.py:419\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    417\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Open Pre-trained Transformers (øPT) models \n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'OPT-125m (l = 12, d = 768, h = 12)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "S, N = getscoresOPT(d,l,h,dh,model)\n",
    "models['OPT-125m'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'OPT-350m (l = 24, d = 1024, h = 16)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "S, N = getscoresOPT(d,l,h,dh,model)\n",
    "models['OPT-350m'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'OPT-1.3b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "S, N = getscoresOPT(d,l,h,dh,model)\n",
    "models['OPT-1.3b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'OPT-2.7b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-2.7b\")\n",
    "S, N = getscoresOPT(d,l,h,dh,model)\n",
    "models['OPT-2.7b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "'OPT-6.7b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\")\n",
    "S, N = getscoresOPT(d,l,h,dh,model)\n",
    "models['OPT-6.7b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "'OPT-13b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\")\n",
    "S, N = getscoresOPT(d,l,h,dh,model)\n",
    "models['OPT-13b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "# 'OPT-30b (l = 24, d = 2048, h = 32)'\n",
    "# l = 24\n",
    "# d = 2048\n",
    "# h = d // dh\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\")\n",
    "# S, N = getscoresOPT(d,l,h,dh,model)\n",
    "# models['OPT-30b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "# print('done')\n",
    "\n",
    "# 'save'\n",
    "# with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "#     pickle.dump(models, file)\n",
    "\n",
    "# 'OPT-66b (l = 24, d = 2048, h = 32)'\n",
    "# l = 24\n",
    "# d = 2048\n",
    "# h = d // dh\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\")\n",
    "# S, N = getscoresOPT(d,l,h,dh,model)\n",
    "# models['OPT-66b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "# print('done')\n",
    "\n",
    "# 'save'\n",
    "# with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "#     pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "T5 models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "    \n",
    "'T5 small model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 64\n",
    "l = 6\n",
    "d = 512\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "S, N = getscoresT5(d,l,h,dh,model)\n",
    "models['T5small'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'T5 base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 64\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "S, N = getscoresT5(d,l,h,dh,model)\n",
    "models['T5base'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'T5 large model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 32\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")\n",
    "S, N = getscoresT5(d,l,h,dh,model)\n",
    "models['T5large'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'T5 3B model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 8\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-3B\")\n",
    "S, N = getscoresT5(d,l,h,dh,model)\n",
    "models['T53b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
