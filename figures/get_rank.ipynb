{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM\n",
    "from transformers import BertModel, AlbertModel, DistilBertModel, RobertaModel, OpenAIGPTModel, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_layers(d,h,dh,Wq,Wk):\n",
    "\n",
    "    RankFull = np.linalg.matrix_rank(Wq @ Wk)\n",
    "    RankHeads = np.zeros(h)\n",
    "\n",
    "    for j, head in enumerate(range(0, d, dh)):\n",
    "        M = Wq[:, head: head + dh] @ Wk[head : head + dh,:]\n",
    "        RankHeads[j] = np.linalg.matrix_rank(M)\n",
    "    \n",
    "    return  RankFull, RankHeads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getscoresBERT(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.encoder.layer[layer].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.layer[layer].attention.self.key.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresDistillBERT(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.transformer.layer[layer].attention.q_lin.weight.detach().numpy()\n",
    "        Wk = model.transformer.layer[layer].attention.k_lin.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresALBERT(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.encoder.albert_layer_groups[layer].albert_layers[0].attention.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.albert_layer_groups[layer].albert_layers[0].attention.key.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresALBERT(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.roberta.encoder.layer[layer].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.roberta.encoder.layer[layer].attention.self.key.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresGPT(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.h[layer].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.h[layer].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresGPTneo(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.transformer.h[layer].attn.attention.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[layer].attn.attention.k_proj.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresGPTj(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.transformer.h[layer].attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[layer].attn.k_proj.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresDistillGPT(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.transformer.h[layer].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.transformer.h[layer].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresOPT(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.model.decoder.layers[layer].self_attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.model.decoder.layers[layer].self_attn.k_proj.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList\n",
    "\n",
    "def getscoresT5(d,l,h,dh,model):\n",
    "\n",
    "    RankFullList = np.zeros(l)\n",
    "    RankHeadsList = np.zeros((l,h))\n",
    "    for layer in range(l):\n",
    "        \n",
    "        Wq = model.encoder.block[layer].layer[0].SelfAttention.q.weight.detach().numpy()\n",
    "        Wk = model.encoder.block[layer].layer[0].SelfAttention.k.weight.detach().numpy()\n",
    "\n",
    "        RankFull, RankHeads = rank_layers(d,h,dh,Wq.T,Wk)\n",
    "        RankFullList[layer] = RankFull\n",
    "        RankHeadsList[layer,:] = RankHeads\n",
    "\n",
    "    return  RankFullList, RankHeadsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- KEY (str): model name\n",
    "- VALUES (list): [layers (int), embedding dim (int), heads (int), head dim (int), S scores, N scores]\n",
    "\"\"\"\n",
    "\n",
    "if os.path.isfile('../data/fig_ranks/models.pkl'):\n",
    "    with open('../data/fig_ranks/models.pkl', 'rb') as file:\n",
    "        models = pickle.load(file)\n",
    "else: models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "BERT models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "dh = 64\n",
    "\n",
    "'BERT tiny (l = 2, d = 128, h = 2 ; 4.40M parameters)'\n",
    "l = 2\n",
    "d = 128\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "full, heads = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTtiny'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'BERT mini (l = 4, d = 256, h = 4 ; 11.3M parameters)'\n",
    "l = 4\n",
    "d = 256\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n",
    "full, heads = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTmini'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'BERT small (l = 4, d = 512, h = 8 ; 29.1M parameters)'\n",
    "l = 4\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-512_A-8\")\n",
    "full, heads = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTsmall'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'BERT medium (l = 8, d = 512, h = 8 ; 41.7M parameters)'\n",
    "l = 8\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-8_H-512_A-8\")\n",
    "full, heads = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTmedium'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'BERT base (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "full, heads = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTbase'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "full, heads = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTlarge'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'BERT large (masking) (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "full, heads = getscoresBERT(d,l,h,dh,model)\n",
    "models['BERTlarge_mask'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'DistillBERT base model (l = 6, d = 768, h = 12 ; tot num parameters 66M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "full, heads = getscoresDistillBERT(d,l,h,dh,model)\n",
    "models['DistillBERT'] = [l,d,h,dh,full,heads]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_ranks/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
