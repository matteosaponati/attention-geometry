{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM\n",
    "from transformers import BertModel, AlbertModel, DistilBertModel, RobertaModel, OpenAIGPTModel, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(A):\n",
    "    \n",
    "    S = np.linalg.norm(.5 * (A + A.T), 'fro') / np.linalg.norm(A, 'fro')\n",
    "    N = np.linalg.norm(.5 * ((A - A.T)), 'fro') / np.linalg.norm(A, 'fro')\n",
    "\n",
    "    return S, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getscoresBERT(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.encoder.layer[i].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.layer[i].attention.self.key.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresDistillBERT(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.layer[i].attention.q_lin.weight.detach().numpy()\n",
    "        Wk = model.transformer.layer[i].attention.k_lin.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresALBERT(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.encoder.albert_layer_groups[i].albert_layers[0].attention.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.albert_layer_groups[i].albert_layers[0].attention.key.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresDistillROBERTA(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.roberta.encoder.layer[i].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.roberta.encoder.layer[i].attention.self.key.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPT(l,d,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.h[i].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.h[i].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPTneo(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.h[i].attn.attention.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[i].attn.attention.k_proj.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPTneox(l,d,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.gpt_neox.layers[i].attention.query_key_value.weight[:d,:].detach().numpy()\n",
    "        Wk = model.gpt_neox.layers[i].attention.query_key_value.weight[d:2*d,:].detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresGPTj(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.h[i].attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[i].attn.k_proj.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresDistillGPT(l,d,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.transformer.h[i].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.transformer.h[i].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresOPT(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.model.decoder.layers[i].self_attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.model.decoder.layers[i].self_attn.k_proj.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N\n",
    "\n",
    "def getscoresT5(l,model):\n",
    "\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        Wq = model.encoder.block[i].layer[0].SelfAttention.q.weight.detach().numpy()\n",
    "        Wk = model.encoder.block[i].layer[0].SelfAttention.k.weight.detach().numpy()\n",
    "        M = Wq @ (Wk.T)\n",
    "        S[i], N[i] = scores(M)   \n",
    "\n",
    "    return  S, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- KEY (str): model name\n",
    "- VALUES (list): [layers (int), embedding dim (int), heads (int), head dim (int), S scores, N scores]\n",
    "\"\"\"\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "BERT models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'BERT tiny (l = 2, d = 128, h = 2 ; 4.40M parameters)'\n",
    "l = 2\n",
    "d = 128\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['BERTtiny'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT mini (l = 4, d = 256, h = 4 ; 11.3M parameters)'\n",
    "l = 4\n",
    "d = 256\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['BERTmini'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT small (l = 4, d = 512, h = 8 ; 29.1M parameters)'\n",
    "l = 4\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-512_A-8\")\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['BERTsmall'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT medium (l = 8, d = 512, h = 8 ; 41.7M parameters)'\n",
    "l = 8\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-8_H-512_A-8\")\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['BERTmedium'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT base (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['BERTbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['BERTlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT large (masking) (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['BERTlarge_mask'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillBERT base model (l = 6, d = 768, h = 12 ; tot num parameters 66M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "S, N = getscoresDistillBERT(l,model)\n",
    "models['DistillBERT'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "ROBERTA models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'ROBERTA base (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['ROBERTAbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ROBERTA large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "S, N = getscoresBERT(l,model)\n",
    "models['ROBERTAlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillROBERTA base (l = 6, d = 768, h = 12 ; tot num parameters 82M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "S, N = getscoresDistillROBERTA(l,model)\n",
    "models['DistillROBERTA'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "ALBERT models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "    \n",
    "dh = 64\n",
    "\n",
    "'ALBERT base model (l = 12, d = 768, h = 12 ; tot num parameters 11M)'\n",
    "l = 1\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "S, N = getscoresALBERT(l,model)\n",
    "models['ALBERTbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT large model (l = 24, d = 1024, h = 16 ; tot num parameters 17M)'\n",
    "l = 1\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-large-v2\")\n",
    "S, N = getscoresALBERT(l,model)\n",
    "models['ALBERTlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT xlarge model (l = 24, d = 2048, h = 16 ; tot num parameters 58M)'\n",
    "dh = 64\n",
    "l = 1\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
    "S, N = getscoresALBERT(l,model)\n",
    "models['ALBERTxlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT xxlarge model (l = 12, d = 4096, h = 64 ; tot num parameters 223M)'\n",
    "dh = 64\n",
    "l = 1\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-xxlarge-v2\")\n",
    "S, N = getscoresALBERT(l,model)\n",
    "models['ALBERTxxlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generative Pre-trained Transformers (GPT) models \n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'GPT 1 (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "S, N = getscoresGPT(l,d,model)\n",
    "models['GPT'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 (l = 12, d = 768, h = 12 ; 117M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "S, N = getscoresGPT(l,d,model)\n",
    "models['GPT2'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 medium (l = 24, d = 1024, h = 16 ; 345M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "S, N = getscoresGPT(l,d,model)\n",
    "models['GPT2medium'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 large (l = 36, d = 1280, h = 20 ; 774M parameters)'\n",
    "l = 36\n",
    "d = 1280\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-large')\n",
    "S, N = getscoresGPT(l,d,model)\n",
    "models['GPT2large'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 xl (l = 48, d = 1600, h = 25 ; 1558M parameters)'\n",
    "l = 48\n",
    "d = 1600\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "S, N = getscoresGPT(l,d,model)\n",
    "models['GPT2xl'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillGPT2 base model (l = 12, d = 768, h = 12 ; tot num parameters 82M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "S, N = getscoresDistillGPT(l,d,model)\n",
    "models['DistillGPT2'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "GPT Neo models (EleutherAI)\n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "'GPT neo 125m (l = 12, d = 768, h = 12)'\n",
    "dh = 64\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "S, N = getscoresGPTneo(l,model)\n",
    "models['GPTneo-125m'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT neo 1.3b (l = 12, d = 768, h = 16)'\n",
    "dh = 128\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "S, N = getscoresGPTneo(l,model)\n",
    "models['GPTneo-1.3b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT neo 2.7b (l = 12, d = 768, h = 20)'\n",
    "dh = 128\n",
    "l = 32\n",
    "d = 2560\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "S, N = getscoresGPTneo(l,model)\n",
    "models['GPTneo-2.7b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "# 'GPT neox 20b (l = 44, d = 768, h = 64)'\n",
    "# dh = 96\n",
    "# l = 44\n",
    "# d = 6144\n",
    "# h = d // dh\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "# S, N = getscoresGPTneox(d,l,h,dh,model)\n",
    "# models['GPTneox-20b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "# with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "#     pickle.dump(models, file)\n",
    "\n",
    "'GPT-j 6b (l = 28, d = 4096, h = 16)'\n",
    "dh = 256\n",
    "l = 28\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
    "S, N = getscoresGPTj(l,model)\n",
    "models['GPTj-6b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 19.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Open Pre-trained Transformers (øPT) models \n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'OPT-125m (l = 12, d = 768, h = 12)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "S, N = getscoresOPT(l,model)\n",
    "models['OPT-125m'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'OPT-350m (l = 24, d = 1024, h = 16)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "S, N = getscoresOPT(l,model)\n",
    "models['OPT-350m'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'OPT-1.3b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "S, N = getscoresOPT(l,model)\n",
    "models['OPT-1.3b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'OPT-2.7b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-2.7b\")\n",
    "S, N = getscoresOPT(l,model)\n",
    "models['OPT-2.7b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "'OPT-6.7b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\")\n",
    "S, N = getscoresOPT(l,model)\n",
    "models['OPT-6.7b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "# 'OPT-13b (l = 24, d = 2048, h = 32)'\n",
    "# l = 24\n",
    "# d = 2048\n",
    "# h = d // dh\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\")\n",
    "# S, N = getscoresOPT(l,model)\n",
    "# models['OPT-13b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "# print('done')\n",
    "\n",
    "# 'save'\n",
    "# with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "#     pickle.dump(models, file)\n",
    "\n",
    "# 'OPT-30b (l = 24, d = 2048, h = 32)'\n",
    "# l = 24\n",
    "# d = 2048\n",
    "# h = d // dh\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\")\n",
    "# S, N = getscoresOPT(d,l,h,dh,model)\n",
    "# models['OPT-30b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "# print('done')\n",
    "\n",
    "# 'save'\n",
    "# with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "#     pickle.dump(models, file)\n",
    "\n",
    "# 'OPT-66b (l = 24, d = 2048, h = 32)'\n",
    "# l = 24\n",
    "# d = 2048\n",
    "# h = d // dh\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\")\n",
    "# S, N = getscoresOPT(d,l,h,dh,model)\n",
    "# models['OPT-66b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "# print('done')\n",
    "\n",
    "# 'save'\n",
    "# with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "#     pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "T5 models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "    \n",
    "'T5 small model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 64\n",
    "l = 6\n",
    "d = 512\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "S, N = getscoresT5(l,model)\n",
    "models['T5small'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'T5 base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 64\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "S, N = getscoresT5(l,model)\n",
    "models['T5base'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'T5 large model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 32\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")\n",
    "S, N = getscoresT5(l,model)\n",
    "models['T5large'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'T5 3B model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 8\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-3B\")\n",
    "S, N = getscoresT5(l,model)\n",
    "models['T53b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/modelsFull.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
