{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM\n",
    "from transformers import BertModel, AlbertModel, DistilBertModel, RobertaModel, BartModel, OpenAIGPTModel, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposition_SN(A):\n",
    "    \n",
    "    S = np.linalg.norm(.5 * (A + A.T), 'fro') / np.linalg.norm(A, 'fro')\n",
    "    N = np.linalg.norm(.5 * (A - A.T), 'fro') / np.linalg.norm(A, 'fro')\n",
    "\n",
    "    return S, N\n",
    "\n",
    "def decomposition_blocks(d,l,h,dh,A):\n",
    "    \n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "\n",
    "    for i, layer in enumerate(range(l)):\n",
    "        for j, head in enumerate(range(0,d,dh)):\n",
    "            S[i,j], N[i,j] = decomposition_SN(A[layer][head:head+dh,head:head+dh])\n",
    "    return  S, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matricesBERT(model):\n",
    "\n",
    "    layers = len(model.encoder.layer)    \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.layer[l].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.layer[l].attention.self.key.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesALBERT(model):\n",
    "\n",
    "    layers = len(model.encoder.albert_layer_groups)    \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.albert_layer_groups[l].albert_layers[0].attention.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.albert_layer_groups[l].albert_layers[0].attention.key.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesDistillBERT(model):\n",
    "\n",
    "    layers = len(model.transformer.layer)\n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.transformer.layer[l].attention.q_lin.weight.detach().numpy()\n",
    "        Wk = model.transformer.layer[l].attention.k_lin.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesBART(model):\n",
    "\n",
    "    layers = len(model.encoder.layers)  \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.layers[l].self_attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.encoder.layers[l].self_attn.k_proj.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesGPT(d,model):\n",
    "\n",
    "    layers = len(model.h)  \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.h[l].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.h[l].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesDistillGPT(d,model):\n",
    "\n",
    "    layers = len(model.transformer.h)  \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.transformer.h[l].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.transformer.h[l].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesDistillROBERTA(model):\n",
    "\n",
    "    layers = len(model.roberta.encoder.layer)    \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.roberta.encoder.layer[l].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.roberta.encoder.layer[l].attention.self.key.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesT5(model):\n",
    "\n",
    "    layers = len(model.encoder.block)\n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.block[l].layer[0].SelfAttention.q.weight.detach().numpy()\n",
    "        Wk = model.encoder.block[l].layer[0].SelfAttention.k.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesOPT(model):\n",
    "\n",
    "    layers = len(model.model.decoder.layers)\n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.model.decoder.layers[l].self_attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.model.decoder.layers[l].self_attn.k_proj.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesGPTneo(model):\n",
    "\n",
    "    layers = len(model.transformer.h)\n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.transformer.h[l].attn.attention.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[l].attn.attention.k_proj.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesGPTneox(d,l,h,dh,model):\n",
    "    \n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "\n",
    "    for i in range(l):\n",
    "        print(l)\n",
    "\n",
    "        Wq = model.gpt_neox.layers[i].attention.query_key_value.weight[:d,:].detach().numpy()\n",
    "        Wk = model.gpt_neox.layers[i].attention.query_key_value.weight[d:2*d,:].detach().numpy()\n",
    "        M = Wq@(Wk.T)\n",
    "\n",
    "        for j, head in enumerate(range(0,d,dh)):\n",
    "            S[i,j], N[i,j] = decomposition_SN(M[head:head+dh,head:head+dh])    \n",
    "\n",
    "    return S ,N\n",
    "\n",
    "def get_matricesGPTj(model):\n",
    "\n",
    "    layers = len(model.transformer.h)\n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.transformer.h[l].attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.transformer.h[l].attn.k_proj.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def decomposition_blocks(d,l,h,dh,A):\n",
    "    \n",
    "    S = np.zeros((l,h))\n",
    "    N = np.zeros((l,h))\n",
    "\n",
    "    for i, layer in enumerate(range(l)):\n",
    "        for j, head in enumerate(range(0,d,dh)):\n",
    "            S[i,j], N[i,j] = decomposition_SN(A[layer][head:head+dh,head:head+dh])\n",
    "    return  S, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- KEY (str): model name\n",
    "- VALUES (list): [layers (int), embedding dim (int), heads (int), head dim (int), S scores, N scores]\n",
    "\"\"\"\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "BERT models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "dh = 64\n",
    "\n",
    "'BERT tiny (l = 2, d = 128, h = 2 ; 4.40M parameters)'\n",
    "l = 2\n",
    "d = 128\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['BERTtiny'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT mini (l = 4, d = 256, h = 4 ; 11.3M parameters)'\n",
    "l = 4\n",
    "d = 256\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['BERTmini'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT small (l = 4, d = 512, h = 8 ; 29.1M parameters)'\n",
    "l = 4\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-512_A-8\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_small, NBert_small = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'BERT medium (l = 8, d = 512, h = 8 ; 41.7M parameters)'\n",
    "l = 8\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-8_H-512_A-8\")\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['BERTmedium'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT base (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['BERTbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['BERTlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'BERT large (masking) (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['BERTlarge_mask'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillBERT base model (l = 6, d = 768, h = 12 ; tot num parameters 66M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "M = get_matricesDistillBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['DistillBERT'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "ROBERTA models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'ROBERTA base (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['ROBERTAbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ROBERTA large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "M = get_matricesBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['ROBERTAlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillROBERTA base (l = 6, d = 768, h = 12 ; tot num parameters 82M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "M = get_matricesDistillROBERTA(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['DistillROBERTA'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "ALBERT models \n",
    " - MODEL: Bidirectional, Encoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    " - METRICS: perplexity, cross-entropy\n",
    "\n",
    " MLM: randomly masked some words in the sentence, predict masked words with cross-entropy \n",
    " over the vocabulary \n",
    " NSP: \n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence, bi-directionally.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "    \n",
    "dh = 64\n",
    "\n",
    "'ALBERT base model (l = 12, d = 768, h = 12 ; tot num parameters 11M)'\n",
    "l = 1\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['ALBERTbase'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT large model (l = 24, d = 1024, h = 16 ; tot num parameters 17M)'\n",
    "l = 1\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-large-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['ALBERTlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT xlarge model (l = 24, d = 2048, h = 16 ; tot num parameters 58M)'\n",
    "dh = 64\n",
    "l = 1\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['ALBERTxlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'ALBERT xxlarge model (l = 12, d = 4096, h = 64 ; tot num parameters 223M)'\n",
    "dh = 64\n",
    "l = 1\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AlbertModel.from_pretrained(\"albert-xxlarge-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['ALBERTxxlarge'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generative Pre-trained Transformers (GPT) models \n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "dh = 64\n",
    "\n",
    "'GPT 1 (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "M = get_matricesGPT(d,model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPT'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 (l = 12, d = 768, h = 12 ; 117M parameters)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "M = get_matricesGPT(d,model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPT2'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 medium (l = 24, d = 1024, h = 16 ; 345M parameters)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "M = get_matricesGPT(d,model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPT2medium'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 large (l = 36, d = 1280, h = 20 ; 774M parameters)'\n",
    "l = 36\n",
    "d = 1280\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-large')\n",
    "M = get_matricesGPT(d,model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPT2large'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT2 xl (l = 48, d = 1600, h = 25 ; 1558M parameters)'\n",
    "l = 48\n",
    "d = 1600\n",
    "h = d // dh\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "M = get_matricesGPT(d,model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPT2xl'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'DistillGPT2 base model (l = 12, d = 768, h = 12 ; tot num parameters 82M)'\n",
    "l = 6\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "M = get_matricesDistillGPT(d,model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['DistillGPT2'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "GPT Neo models (EleutherAI)\n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "with open('../data/fig_scores/models.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "'GPT neo 125m (l = 12, d = 768, h = 12)'\n",
    "dh = 64\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "M = get_matricesGPTneo(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPTneo-125m'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT neo 1.3b (l = 12, d = 768, h = 16)'\n",
    "dh = 128\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "M = get_matricesGPTneo(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPTneo-1.3b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT neo 2.7b (l = 12, d = 768, h = 20)'\n",
    "dh = 128\n",
    "l = 32\n",
    "d = 2560\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "M = get_matricesGPTneo(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPTneo-2.7b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT neox 20b (l = 44, d = 768, h = 64)'\n",
    "dh = 96\n",
    "l = 44\n",
    "d = 6144\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "S, N = get_matricesGPTneox(d,l,h,dh,model)\n",
    "models['GPTneox-20b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'GPT-j 6b (l = 28, d = 4096, h = 16)'\n",
    "dh = 256\n",
    "l = 28\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
    "M = get_matricesGPTj(model)\n",
    "S, N = decomposition_blocks(d,l,h,dh,M)\n",
    "models['GPTj-6b'] = [l,d,h,dh,S,N]\n",
    "\n",
    "'save'\n",
    "with open('../data/fig_scores/models.pkl', 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.51s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 30.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m\n\u001b[1;32m     65\u001b[0m h \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m dh\n\u001b[0;32m---> 66\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/opt-30b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m M \u001b[38;5;241m=\u001b[39m get_matricesOPT(model)\n\u001b[1;32m     68\u001b[0m SOPT30b, NOPT30b \u001b[38;5;241m=\u001b[39m decomposition_blocks(d,l,h,dh,M)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/modeling_utils.py:2876\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2876\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:823\u001b[0m, in \u001b[0;36mOPTForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mOPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;66;03m# the lm_head weight is automatically tied to the embed tokens weight\u001b[39;00m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mword_embed_proj_dim, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:755\u001b[0m, in \u001b[0;36mOPTModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: OPTConfig):\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mOPTDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:520\u001b[0m, in \u001b[0;36mOPTDecoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[43m[\u001b[49m\u001b[43mOPTDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:520\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mOPTDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:280\u001b[0m, in \u001b[0;36mOPTDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mOPTAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_layer_norm_before \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdo_layer_norm_before\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdropout\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:148\u001b[0m, in \u001b[0;36mOPTAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, is_decoder, bias)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;241m=\u001b[39m is_decoder\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim, bias\u001b[38;5;241m=\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/torch/nn/init.py:419\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    417\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Open Pre-trained Transformers (øPT) models \n",
    " - MODEL: Unidirectional (causal), Decoder-only Transformer\n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "The idea is that these models have a better understanding of context, where each word is represented as a \n",
    "linear combination of all the other words in the sentence.\n",
    "\"\"\"\n",
    "dh = 64\n",
    "\n",
    "'OPT-125m (l = 12, d = 768, h = 12)'\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT125m, NOPT125m = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'OPT-350m (l = 24, d = 1024, h = 16)'\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT350m, NOPT350m = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'OPT-1.3b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT1_3b, NOPT1_3b = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'OPT-2.7b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-2.7b\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT2_7b, NOPT2_7b = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'OPT-6.7b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT6_7b, NOPT6_7b = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'OPT-13b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT13b, NOPT13b = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "print('done')\n",
    "\n",
    "'OPT-30b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT30b, NOPT30b = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "print('done')\n",
    "\n",
    "'OPT-66b (l = 24, d = 2048, h = 32)'\n",
    "l = 24\n",
    "d = 2048\n",
    "h = d // dh\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\")\n",
    "M = get_matricesOPT(model)\n",
    "SOPT66b, NOPT66b = decomposition_blocks(d,l,h,dh,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "T5 models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "\n",
    "'T5 small model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 64\n",
    "\n",
    "l = 6\n",
    "d = 512\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_small, NT5_small = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'T5 base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 64\n",
    "\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_base, NT5_base = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'T5 large model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 32\n",
    "\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_large, NT5_large = decomposition_blocks(d,l,h,dh,M)\n",
    "\n",
    "'T5 3B model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "dh = 8\n",
    "\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-3B\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_3B, NT5_3B = decomposition_blocks(d,l,h,dh,M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
