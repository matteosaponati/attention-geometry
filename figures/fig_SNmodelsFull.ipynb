{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analysis pipeline on BERT models\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM\n",
    "from transformers import BertModel, AlbertModel, DistilBertModel, RobertaModel, BartModel, OpenAIGPTModel, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposition_SN(A):\n",
    "    \n",
    "    S = np.linalg.norm(.5 * (A + A.T), 'fro') / np.linalg.norm(A, 'fro')\n",
    "    N = np.linalg.norm(.5 * ((A - A.T)), 'fro') / np.linalg.norm(A, 'fro')\n",
    "\n",
    "    return S, N\n",
    "\n",
    "def decomposition_blocks(l,A):\n",
    "    S = np.zeros(l)\n",
    "    N = np.zeros(l)\n",
    "\n",
    "    for i, layer in enumerate(range(l)):\n",
    "        S[i], N[i] = decomposition_SN(A[layer])\n",
    "    return  S, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matricesBERT(model):\n",
    "\n",
    "    layers = len(model.encoder.layer)    \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.layer[l].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.layer[l].attention.self.key.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesALBERT(model):\n",
    "\n",
    "    layers = len(model.encoder.albert_layer_groups)    \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.albert_layer_groups[l].albert_layers[0].attention.query.weight.detach().numpy()\n",
    "        Wk = model.encoder.albert_layer_groups[l].albert_layers[0].attention.key.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesDistillBERT(model):\n",
    "\n",
    "    layers = len(model.transformer.layer)\n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.transformer.layer[l].attention.q_lin.weight.detach().numpy()\n",
    "        Wk = model.transformer.layer[l].attention.k_lin.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesBART(model):\n",
    "\n",
    "    layers = len(model.encoder.layers)  \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.layers[l].self_attn.q_proj.weight.detach().numpy()\n",
    "        Wk = model.encoder.layers[l].self_attn.k_proj.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesGPT(d,model):\n",
    "\n",
    "    layers = len(model.h)  \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.h[l].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.h[l].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesDistillGPT(d,model):\n",
    "\n",
    "    layers = len(model.transformer.h)  \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.transformer.h[l].attn.c_attn.weight[:,:d].detach().numpy()\n",
    "        Wk = model.transformer.h[l].attn.c_attn.weight[:,d:2*d].detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesDistillROBERTA(model):\n",
    "\n",
    "    layers = len(model.roberta.encoder.layer)    \n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.roberta.encoder.layer[l].attention.self.query.weight.detach().numpy()\n",
    "        Wk = model.roberta.encoder.layer[l].attention.self.key.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M\n",
    "\n",
    "def get_matricesT5(model):\n",
    "\n",
    "    layers = len(model.encoder.block)\n",
    "    M = []\n",
    "    for l in range(layers):\n",
    "        Wq = model.encoder.block[l].layer[0].SelfAttention.q.weight.detach().numpy()\n",
    "        Wk = model.encoder.block[l].layer[0].SelfAttention.k.weight.detach().numpy()\n",
    "        M.append(Wq@(Wk.T))        \n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "BERT models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "l = 2\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_tiny, NBert_tiny = decomposition_blocks(l,M)\n",
    "\n",
    "l = 4\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_mini, NBert_mini = decomposition_blocks(l,M)\n",
    "\n",
    "l = 4\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-512_A-8\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_small, NBert_small = decomposition_blocks(l,M)\n",
    "\n",
    "l = 8\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-8_H-512_A-8\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_medium, NBert_medium = decomposition_blocks(l,M)\n",
    "\n",
    "l = 12\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_base, NBert_base = decomposition_blocks(l,M)\n",
    "\n",
    "l = 24\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_large, NBert_large = decomposition_blocks(l,M)\n",
    "\n",
    "l = 24\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "M = get_matricesBERT(model)\n",
    "SBert_largeMasking, NBert_largeMasking = decomposition_blocks(l,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generative Pre-trained Transformer (GPT)\n",
    "\n",
    "\n",
    "Fine-tuning approach: introduce minimal, task-specific parameters, and is trained on downstream tasks by fine-tuning all pre-trained parameters\n",
    "\"\"\"\n",
    "\n",
    "'GPT 1'\n",
    "l = 12\n",
    "d = 768\n",
    "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "M = get_matricesGPT(d,model)\n",
    "SGPT, NGPT = decomposition_blocks(l,M)\n",
    "\n",
    "'GPT2'\n",
    "l = 12\n",
    "d = 768\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "M = get_matricesGPT(d,model)\n",
    "SGPT2, NGPT2 = decomposition_blocks(l,M)\n",
    "\n",
    "'GPT2 medium'\n",
    "l = 24\n",
    "d = 1024\n",
    "model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "M = get_matricesGPT(d,model)\n",
    "SGPT2_medium, NGPT2_medium = decomposition_blocks(l,M)\n",
    "\n",
    "'GPT2 large'\n",
    "l = 36\n",
    "model = GPT2Model.from_pretrained('gpt2-large')\n",
    "M = get_matricesGPT(d,model)\n",
    "SGPT2_large, NGPT2_large = decomposition_blocks(l,M)\n",
    "\n",
    "'GPT2 xl'\n",
    "l = 48\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "M = get_matricesGPT(d,model)\n",
    "SGPT2_xl, NGPT2_xl = decomposition_blocks(l,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "ROBERTA models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "\n",
    "l = 12\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "M = get_matricesBERT(model)\n",
    "SROBERTA_base, NROBERTA_base = decomposition_blocks(l,M)\n",
    "\n",
    "l = 24\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "M = get_matricesBERT(model)\n",
    "SROBERTA_large, NROBERTA_large = decomposition_blocks(l,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "ALBERT models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "\n",
    "'ALBERT base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 1\n",
    "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "SALBERT_base, NALBERT_base = decomposition_blocks(l,M)\n",
    "\n",
    "'ALBERT large model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 1\n",
    "model = AlbertModel.from_pretrained(\"albert-large-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "SALBERT_large, NALBERT_large = decomposition_blocks(l,M)\n",
    "\n",
    "'ALBERT xlarge model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 1\n",
    "model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "SALBERT_xlarge, NALBERT_xlarge = decomposition_blocks(l,M)\n",
    "\n",
    "'ALBERT xxlarge model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 1\n",
    "model = AlbertModel.from_pretrained(\"albert-xxlarge-v2\")\n",
    "M = get_matricesALBERT(model)\n",
    "SALBERT_xxlarge, NALBERT_xxlarge = decomposition_blocks(l,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "DISTILL models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "\n",
    "'DistillBERT base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 6\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "M = get_matricesDistillBERT(model)\n",
    "SDistillBERT, NDistillBERT = decomposition_blocks(l,M)\n",
    "\n",
    "\n",
    "'DistillGPT2 base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 6\n",
    "d = 768\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "M = get_matricesDistillGPT(d,model)\n",
    "SDistillGPT2, NDistillGPT2 = decomposition_blocks(l,M)\n",
    "\n",
    "\n",
    "'DistillROBERTA base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 6\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "M = get_matricesDistillROBERTA(model)\n",
    "SDistillROBERTA, NDistillROBERTA = decomposition_blocks(l,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "T5 models \n",
    " - MODEL: \n",
    " - DATASETS: BookCorpus & English Wikipedia\n",
    " - OBJECTIVES: Masked Language Modeling (MLM), Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT base (l = 12, d = 768, h = 12 ; 110M parameters)\n",
    "\"\"\"\n",
    "\n",
    "'T5 small model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 6\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_small, NT5_small = decomposition_blocks(l,M)\n",
    "\n",
    "'T5 base model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 12\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_base, NT5_base = decomposition_blocks(l,M)\n",
    "\n",
    "'T5 large model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 24\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_large, NT5_large = decomposition_blocks(l,M)\n",
    "\n",
    "'T5 3B model (l = 12, d = 768, h = 12 ; tot num parameters 110M)'\n",
    "l = 24\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-3B\")\n",
    "M = get_matricesT5(model)\n",
    "ST5_3B, NT5_3B = decomposition_blocks(l,M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
