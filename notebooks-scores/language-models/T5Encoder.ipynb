{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelWithLMHead\n",
    "from utils.funs import create_dict\n",
    "from utils.scores import get_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, dir = create_dict('language-models', 'T5Encoder.pkl')\n",
    "\n",
    "path = [\"encoder.block[\", \n",
    "        \"].layer[0].SelfAttention.q.weight\", \n",
    "        \"].layer[0].SelfAttention.k.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:1682: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'T5 small model (l = 6, d = 512, h = 8 ; tot num parameters 60.5M)'\n",
    "dh = 64\n",
    "l = 6\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "models = get_information(models, model, 't5-small', d, l, h, dh, path)\n",
    "\n",
    "'T5 small model (l = 6, d = 512, h = 8 ; tot num parameters 60.5M)'\n",
    "dh = 64\n",
    "l = 6\n",
    "d = 512\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-v1_1-small\")\n",
    "models = get_information(models, model, 't5-small-v1.1', d, l, h, dh, path)\n",
    "\n",
    "'T5 base model (l = 12, d = 768, h = 12 ; tot num parameters 223M)'\n",
    "dh = 64\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "models = get_information(models, model, 't5-base', d, l, h, dh, path)\n",
    "\n",
    "'T5 base model (l = 12, d = 768, h = 12 ; tot num parameters 223M)'\n",
    "dh = 64\n",
    "l = 12\n",
    "d = 768\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-v1_1-base\")\n",
    "models = get_information(models, model, 't5-base-v1.1', d, l, h, dh, path)\n",
    "\n",
    "'T5 large model (l = 24, d = 1024, h = 16 ; tot num parameters 738M)'\n",
    "dh = 64\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")\n",
    "models = get_information(models, model, 't5-large', d, l, h, dh, path)\n",
    "\n",
    "'T5 large model (l = 24, d = 1024, h = 16 ; tot num parameters 738M)'\n",
    "dh = 64\n",
    "l = 24\n",
    "d = 1024\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-v1_1-large\")\n",
    "models = get_information(models, model, 't5-large-v1.1', d, l, h, dh, path)\n",
    "\n",
    "'T5 3b model (l = 24, d = 4096, h = 32 ; tot num parameters 2.85B)'\n",
    "## Wq and Wk have dim [4096, 1024], must use some sort of group attention\n",
    "dh = 128\n",
    "l = 24\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AutoModelWithLMHead.from_pretrained(\"google-t5/t5-3b\")\n",
    "models = get_information(models, model, 't5-3b', d, l, h, dh, path)\n",
    "\n",
    "'T5 3b model (l = 24, d = 4096, h = 32 ; tot num parameters 2.85B)'\n",
    "## Wq and Wk have dim [4096, 1024], must use some sort of group attention\n",
    "dh = 128\n",
    "l = 24\n",
    "d = 4096\n",
    "h = d // dh\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-v1_1-xl\")\n",
    "models = get_information(models, model, 't5-3b-v1.1', d, l, h, dh, path)\n",
    "\n",
    "'T5 11b model (l = 24, d = 16384, h = 32 ; tot num parameters 11B)'\n",
    "## Wq and Wk have dim [4096, 1024], must use some sort of group attention\n",
    "dh = 64\n",
    "l = 24\n",
    "d = 16384\n",
    "h = d // dh\n",
    "model = AutoModelWithLMHead.from_pretrained(\"google-t5/t5-11b\")\n",
    "models = get_information(models, model, 't5-11b-v1.1', d, l, h, dh, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'save'\n",
    "with open(dir, 'wb') as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
