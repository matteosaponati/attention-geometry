{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### numerical evaluation for the time complexity of computing the symmetry score\n",
    "## -----------\n",
    "### - d:   embedding dimension\n",
    "### - d_h: head dimension\n",
    "\n",
    "## method with numpy.linalg.norm\n",
    "## ----------\n",
    "## - compute the M matrix explicitly -> O(d^2 d_h)\n",
    "## - compute M + M.T \n",
    "## - compute np.linalg.norm for M and .5 * (M + M.T)\n",
    "## - compute the ratio of the two norms\n",
    "def scores_norm(Wq, Wk):\n",
    "    \n",
    "    M = Wq @ Wk.T\n",
    "    r = np.linalg.norm(.5 * (M + M.T), 'fro')**2 / np.linalg.norm(M, 'fro')**2\n",
    "\n",
    "    return r\n",
    "\n",
    "## method with np.einsum (leverage properties of M and cyclicity of trace)\n",
    "## ----------\n",
    "## - compute A, B, and C (without computing M explicitly)  -> O(d d_h^2)\n",
    "## - compute np.einsum of C with itself (fast way to compute the trace)\n",
    "## - compute np.einsum of A and B (fast way to compute the trace)\n",
    "## - compute the ratio of the two norms\n",
    "\n",
    "def scores_trace(Wq, Wk):\n",
    "     \n",
    "    ## (k x n) @ (n x k) -> O(nk^2)\n",
    "    A = Wq.T @ Wq\n",
    "    B = Wk.T @ Wk\n",
    "    C = Wk.T @ Wq\n",
    "\n",
    "    r = .5 * (1 + (np.einsum('ij,ji->', C, C) / np.einsum('ij,ji->', A, B)))\n",
    "\n",
    "    return r\n",
    "\n",
    "def scores_trace_heads(Wq, Wk):\n",
    "     \n",
    "    ## (h x k x n) @ (h x n x k) -> O(nk^2)\n",
    "    A = Wq.T @ Wq\n",
    "    B = Wk.T @ Wk\n",
    "    C = Wk.T @ Wq\n",
    "\n",
    "    r = .5 * (1 + (np.einsum('ij,ji->', C, C) / np.einsum('ij,ji->', A, B)))\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## test with BERT large\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "l = 14\n",
    "dh = 64\n",
    "d = 1024\n",
    "h = d // dh\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "Wq = model.encoder.layer[0].attention.self.query._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "Wk = model.encoder.layer[0].attention.self.key._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5953140874939474\n",
      "0.5953149423003197\n",
      "3.78 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "593 µs ± 38.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5205281040301025\n",
      "0.5205290913581848\n",
      "3.95 ms ± 365 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "690 µs ± 64.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5762675599554818\n",
      "0.5762671381235123\n",
      "3.95 ms ± 208 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "826 µs ± 252 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5275472278991179\n",
      "0.5275460779666901\n",
      "4.16 ms ± 467 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "703 µs ± 44.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5186810611568575\n",
      "0.5186807066202164\n",
      "4.09 ms ± 246 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "800 µs ± 107 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.6056445305491756\n",
      "0.6056437641382217\n",
      "3.76 ms ± 579 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "601 µs ± 24.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.7040895503956844\n",
      "0.7040895819664001\n",
      "3.66 ms ± 115 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "572 µs ± 29.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5424157335578136\n",
      "0.5424153469502926\n",
      "3.57 ms ± 185 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "562 µs ± 18.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5100294392195002\n",
      "0.5100303962826729\n",
      "3.69 ms ± 199 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "615 µs ± 62.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.650582003359195\n",
      "0.6505829989910126\n",
      "3.69 ms ± 254 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "572 µs ± 18.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5411443632849717\n",
      "0.5411441810429096\n",
      "3.78 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "572 µs ± 21.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.6507541645532593\n",
      "0.6507547497749329\n",
      "3.83 ms ± 247 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "564 µs ± 15 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.7321478148572135\n",
      "0.7321491837501526\n",
      "3.7 ms ± 258 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "581 µs ± 29.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5443709631683488\n",
      "0.544371135532856\n",
      "3.66 ms ± 246 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "561 µs ± 19.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "for j in range(l):\n",
    "    \n",
    "    print(scores_norm(Wq[:,j,:], Wk[:,j,:]))\n",
    "    print(scores_trace(Wq[:,j,:], Wk[:,j,:]))\n",
    "\n",
    "    %timeit scores_norm(Wq[:,j,:], Wk[:,j,:])\n",
    "    %timeit scores_trace(Wq[:,j,:], Wk[:,j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## test with LLAMA 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "'LLAMA 2 7b (l = 32, d = 4096, h = 32 ; tot num parameters 7B)'\n",
    "dh = 128\n",
    "l = 32\n",
    "d = 4096\n",
    "h = 32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "Wq = model.model.layers[0].self_attn.q_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "Wk = model.model.layers[0].self_attn.k_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5261237925015685\n",
      "0.5260768656918057\n",
      "123 ms ± 5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "4.94 ms ± 622 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    \n",
    "    print(scores_norm(Wq[:,j,:], Wk[:,j,:]))\n",
    "    print(scores_trace(Wq[:,j,:], Wk[:,j,:]))\n",
    "\n",
    "    %timeit scores_norm(Wq[:,j,:], Wk[:,j,:])\n",
    "    %timeit scores_trace(Wq[:,j,:], Wk[:,j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### numerical evaluation for grouping the computation of the symmetry scores\n",
    "### across heads and layers\n",
    "### -----------\n",
    "### - d:        embedding dimension\n",
    "### - d_h:      head dimension\n",
    "### - layers:   # of layers\n",
    "\n",
    "def scores_trace_full(layers):\n",
    "\n",
    "    Wq, Wk = stack_layers(layers)\n",
    "\n",
    "    A = np.einsum('lnhi,lnhj->lhij', Wq, Wq)\n",
    "    B = np.einsum('lnhi,lnhj->lhij', Wk, Wk)\n",
    "    C = np.einsum('lnhi,lnhj->lhij', Wk, Wq)\n",
    "\n",
    "    r = 0.5 * (1 + np.einsum('lhij,lhji->lh', C, C) / np.einsum('lhij,lhji->lh', A, B))\n",
    "\n",
    "    return r\n",
    "\n",
    "def scores_trace_loop(h,l,layers):\n",
    "\n",
    "    Wq, Wk = stack_layers(layers)\n",
    "\n",
    "    r = []\n",
    "\n",
    "    for i in range(l):\n",
    "        for j in range(h):        \n",
    "\n",
    "            r.append(scores_trace(Wq[i,:,j,:], Wk[i,:,j,:]))\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test with TInyGPT 8M parameters\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "'TinyGPT 8m (l = 8, d = 64, h = 4 ; 8M parameters)'\n",
    "dh = 64\n",
    "l = 8\n",
    "d = 256\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-8M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_layers(layers):\n",
    "\n",
    "    WqList = []\n",
    "    WkList = []\n",
    "    for i, layer in enumerate(layers):\n",
    "\n",
    "        self_attention = layer.attn.attention\n",
    "\n",
    "        Wq = self_attention.q_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "        Wk = self_attention.k_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "\n",
    "        WqList.append(Wq)\n",
    "        WkList.append(Wk)\n",
    "\n",
    "    WqList = np.stack(WqList,axis=0)\n",
    "    WkList = np.stack(WkList,axis=0)\n",
    "\n",
    "    return WqList, WkList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51008594 0.5002863  0.5011067  0.5099732 ]\n",
      " [0.51710474 0.48365727 0.5046482  0.5378101 ]\n",
      " [0.41431645 0.4889804  0.44898984 0.4257167 ]\n",
      " [0.47386387 0.52357817 0.47452947 0.52988505]\n",
      " [0.5297684  0.41397804 0.38179463 0.39657295]\n",
      " [0.45648208 0.4570157  0.50635654 0.40751418]\n",
      " [0.5407863  0.56023765 0.5436243  0.5434173 ]\n",
      " [0.5298016  0.5333205  0.5603063  0.5317767 ]]\n",
      "20.9 ms ± 184 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "[0.5100859254598618, 0.5002863103582058, 0.5011066528968513, 0.5099732391536236, 0.5171047709882259, 0.4836572855710983, 0.5046482090838253, 0.537810105830431, 0.41431643068790436, 0.48898041900247335, 0.4489898346364498, 0.42571668326854706, 0.4738638661801815, 0.5235781688243151, 0.47452946938574314, 0.5298850629478693, 0.5297684259712696, 0.41397804766893387, 0.38179463893175125, 0.39657294005155563, 0.45648208260536194, 0.4570157080888748, 0.5063565503805876, 0.40751418471336365, 0.5407863259315491, 0.5602376125752926, 0.5436243042349815, 0.5434172973036766, 0.5298016350716352, 0.5333204753696918, 0.5603063218295574, 0.531776737421751]\n",
      "10.1 ms ± 4.76 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "layers = model.transformer.h\n",
    "\n",
    "print(scores_trace_full(layers))\n",
    "%timeit scores_trace_full(layers)\n",
    "\n",
    "print(scores_trace_loop(h,l,layers))\n",
    "%timeit scores_trace_loop(h,l,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test with BERT large\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "dh = 64\n",
    "d = 1024\n",
    "h = d // dh\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_layers(layers):\n",
    "\n",
    "    WqList = []\n",
    "    WkList = []\n",
    "    for i, layer in enumerate(layers):\n",
    "\n",
    "        # access self-attention module within layer\n",
    "        self_attention = layer.attention.self\n",
    "\n",
    "        # access parameters of Conv1D\n",
    "        Wq = self_attention.query._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "        Wk = self_attention.key._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "\n",
    "        WqList.append(Wq)\n",
    "        WkList.append(Wk)\n",
    "\n",
    "    WqList = np.stack(WqList,axis=0)\n",
    "    WkList = np.stack(WkList,axis=0)\n",
    "\n",
    "    return WqList, WkList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5953149  0.5205291  0.5762671  0.52754605 0.5186807  0.60564375\n",
      "  0.7040896  0.5424154  0.5100304  0.650583   0.5411442  0.65075475\n",
      "  0.7321492  0.5443711  0.51514816 0.506263  ]\n",
      " [0.6182766  0.7586884  0.6002054  0.65335536 0.71380043 0.53553736\n",
      "  0.59238815 0.52918947 0.59624547 0.5107676  0.55948055 0.5727372\n",
      "  0.52321815 0.6255044  0.5412552  0.5369339 ]\n",
      " [0.5764037  0.5160598  0.6525016  0.5320777  0.6415573  0.5513461\n",
      "  0.5023911  0.5294934  0.740103   0.50999993 0.52891695 0.47562772\n",
      "  0.5361897  0.6094643  0.5196909  0.53254163]\n",
      " [0.54349697 0.64425504 0.52394927 0.5461073  0.53182757 0.55093443\n",
      "  0.6887869  0.5313994  0.51421505 0.40598178 0.5647216  0.5434591\n",
      "  0.62864274 0.66327137 0.7294234  0.533965  ]\n",
      " [0.5858928  0.5748431  0.6538075  0.6340821  0.52380526 0.59078705\n",
      "  0.5359635  0.5951824  0.55511755 0.5569792  0.52062106 0.5553155\n",
      "  0.78356344 0.5621277  0.5184722  0.53117317]\n",
      " [0.5476827  0.5499055  0.57550657 0.53467035 0.61678123 0.61833227\n",
      "  0.62975156 0.6150202  0.59970033 0.52908796 0.5269246  0.5367606\n",
      "  0.5634233  0.5659909  0.5399018  0.5834232 ]\n",
      " [0.5547347  0.5276153  0.5614695  0.5584613  0.6607797  0.55616146\n",
      "  0.5289778  0.5395324  0.54300636 0.61571944 0.5233395  0.66740495\n",
      "  0.695529   0.5288877  0.5268692  0.5817586 ]\n",
      " [0.5228045  0.56322175 0.5226674  0.5243558  0.52485454 0.6108639\n",
      "  0.5187575  0.5095119  0.53238547 0.56324035 0.5700833  0.5533981\n",
      "  0.50371844 0.5402525  0.5271512  0.6001209 ]\n",
      " [0.51258606 0.53696597 0.5376831  0.53042823 0.55460984 0.5587268\n",
      "  0.5481387  0.6467939  0.5103906  0.5726125  0.5828199  0.62128675\n",
      "  0.53056395 0.62180877 0.50825346 0.54722464]\n",
      " [0.6513547  0.5220149  0.67522895 0.5505453  0.53476584 0.5517807\n",
      "  0.57367903 0.563309   0.5673287  0.6426763  0.45014596 0.5342896\n",
      "  0.52279526 0.51788586 0.5303608  0.5286391 ]\n",
      " [0.57215303 0.6015105  0.41344985 0.60022706 0.48814005 0.6168997\n",
      "  0.53136486 0.6400615  0.5753323  0.5947888  0.5425394  0.45864838\n",
      "  0.54728913 0.5342927  0.63149625 0.5628706 ]\n",
      " [0.5683804  0.7347393  0.5295459  0.637733   0.57846206 0.54459363\n",
      "  0.50773966 0.55154526 0.5306785  0.5207677  0.5726569  0.53099275\n",
      "  0.74259484 0.8053928  0.45496365 0.5558038 ]\n",
      " [0.6045047  0.5465598  0.57824194 0.5249974  0.5156664  0.5320432\n",
      "  0.5235552  0.5189566  0.52164584 0.5752684  0.5333745  0.5376302\n",
      "  0.5415689  0.5338249  0.53567576 0.5037091 ]\n",
      " [0.59640926 0.5089039  0.5136173  0.56488407 0.5657716  0.5039567\n",
      "  0.52569234 0.4915079  0.5324356  0.58325565 0.5151026  0.510531\n",
      "  0.4884646  0.58592415 0.52232504 0.50672466]\n",
      " [0.48627132 0.5670992  0.5341644  0.57213485 0.55673206 0.6262858\n",
      "  0.50982505 0.51732314 0.52763903 0.5380222  0.5161523  0.50782925\n",
      "  0.63102573 0.56838596 0.5838747  0.5046706 ]\n",
      " [0.5236387  0.5246789  0.52932143 0.5326176  0.5313436  0.5765718\n",
      "  0.5213884  0.5196628  0.5524155  0.51933235 0.5200833  0.5480819\n",
      "  0.51322156 0.69752043 0.5477457  0.6899096 ]\n",
      " [0.52811265 0.5633628  0.5748334  0.5779023  0.6247947  0.5589268\n",
      "  0.5546406  0.61091053 0.55474925 0.6722989  0.49281365 0.52012396\n",
      "  0.5314955  0.58427316 0.5280517  0.55546117]\n",
      " [0.6014956  0.5765377  0.5517705  0.7133555  0.62524116 0.4644207\n",
      "  0.541967   0.55141586 0.58724064 0.60338897 0.5758573  0.58891344\n",
      "  0.5801778  0.5883791  0.66593605 0.5253997 ]\n",
      " [0.56363356 0.52293533 0.6374306  0.5526958  0.57471424 0.5946033\n",
      "  0.5443726  0.5326544  0.6550645  0.541617   0.5307392  0.56986356\n",
      "  0.5156801  0.5332819  0.6153699  0.53562635]\n",
      " [0.5731989  0.5465693  0.5178597  0.53570855 0.5433635  0.54073656\n",
      "  0.54314977 0.6264071  0.5205477  0.6180135  0.5549618  0.5647554\n",
      "  0.5921571  0.5582222  0.5055714  0.54573065]\n",
      " [0.5717311  0.57691157 0.52207154 0.539683   0.5624242  0.5163685\n",
      "  0.5416357  0.65821856 0.5435864  0.63888925 0.6026622  0.55346054\n",
      "  0.5711109  0.5559644  0.5861784  0.5326707 ]\n",
      " [0.5276768  0.52019954 0.57768714 0.52418643 0.53705806 0.58572364\n",
      "  0.6423406  0.5718651  0.5439769  0.61777806 0.5046329  0.542183\n",
      "  0.66433966 0.5570311  0.5634449  0.556409  ]\n",
      " [0.5698209  0.5971708  0.60107607 0.57921636 0.55437046 0.5314503\n",
      "  0.56489575 0.5496435  0.5480999  0.5594596  0.545747   0.563118\n",
      "  0.59598756 0.5622587  0.5529405  0.5805241 ]\n",
      " [0.5775815  0.5856597  0.61696243 0.5822945  0.5809523  0.64188516\n",
      "  0.57404137 0.5594379  0.53401375 0.6170032  0.56752115 0.5878542\n",
      "  0.59514534 0.5796689  0.58582157 0.5380105 ]]\n",
      "1.48 s ± 2.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "[0.5953149423003197, 0.5205290913581848, 0.5762671381235123, 0.5275460779666901, 0.5186807066202164, 0.6056437641382217, 0.7040895819664001, 0.5424153469502926, 0.5100303962826729, 0.6505829989910126, 0.5411441810429096, 0.6507547497749329, 0.7321491837501526, 0.544371135532856, 0.515148158185184, 0.5062629957683384, 0.6182766035199165, 0.758688360452652, 0.6002054139971733, 0.6533554047346115, 0.713800460100174, 0.5355373360216618, 0.5923881158232689, 0.5291894413530827, 0.5962454825639725, 0.5107675520703197, 0.559480581432581, 0.5727372169494629, 0.5232181325554848, 0.6255043596029282, 0.5412551984190941, 0.5369339287281036, 0.5764036700129509, 0.5160597953945398, 0.6525015532970428, 0.532077718526125, 0.6415572613477707, 0.5513461194932461, 0.5023910913150758, 0.5294933672994375, 0.7401030361652374, 0.509999917820096, 0.5289169456809759, 0.47562771663069725, 0.5361896604299545, 0.6094643026590347, 0.5196908451616764, 0.532541636377573, 0.5434969626367092, 0.6442550867795944, 0.5239492524415255, 0.5461073108017445, 0.5318275801837444, 0.5509344339370728, 0.6887868940830231, 0.5313994139432907, 0.5142150409519672, 0.40598177909851074, 0.5647215768694878, 0.5434591360390186, 0.628642737865448, 0.6632713675498962, 0.7294234484434128, 0.5339649803936481, 0.5858928114175797, 0.5748430863022804, 0.6538075357675552, 0.6340820640325546, 0.5238052885979414, 0.5907870754599571, 0.5359634533524513, 0.5951824188232422, 0.5551175139844418, 0.5569792091846466, 0.5206210631877184, 0.5553155243396759, 0.7835634052753448, 0.5621277242898941, 0.518472209572792, 0.5311731714755297, 0.5476826727390289, 0.5499054603278637, 0.5755065456032753, 0.5346703678369522, 0.6167812123894691, 0.6183322668075562, 0.629751592874527, 0.615020252764225, 0.5997003465890884, 0.5290879793465137, 0.5269246269017458, 0.5367606207728386, 0.5634232833981514, 0.5659909248352051, 0.5399017743766308, 0.5834231823682785, 0.5547346770763397, 0.5276152957230806, 0.5614695027470589, 0.5584613420069218, 0.660779669880867, 0.5561614595353603, 0.5289778020232916, 0.5395324267446995, 0.5430063605308533, 0.6157194450497627, 0.5233395025134087, 0.667404979467392, 0.6955289989709854, 0.5288877040147781, 0.5268691964447498, 0.581758588552475, 0.5228044912219048, 0.5632217228412628, 0.5226674135774374, 0.5243558436632156, 0.5248545724898577, 0.6108639389276505, 0.5187575034797192, 0.5095119029283524, 0.532385490834713, 0.5632403343915939, 0.5700832977890968, 0.5533980578184128, 0.5037184194661677, 0.5402525216341019, 0.5271512195467949, 0.6001208946108818]\n",
      "96.4 ms ± 6.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "layers = model.encoder.layer\n",
    "\n",
    "print(scores_trace_full(layers))\n",
    "%timeit scores_trace_full(layers)\n",
    "\n",
    "print(scores_trace_loop(h,l,layers))\n",
    "%timeit scores_trace_loop(h,l,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.09s/it]\n"
     ]
    }
   ],
   "source": [
    "## test with LLAMA 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "'LLAMA 2 7b (l = 32, d = 4096, h = 32 ; tot num parameters 7B)'\n",
    "dh = 128\n",
    "l = 32\n",
    "d = 4096\n",
    "h = 32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_layers(layers):\n",
    "\n",
    "    WqList = []\n",
    "    WkList = []\n",
    "    for i, layer in enumerate(layers):\n",
    "\n",
    "        # access self-attention module within layer\n",
    "        self_attention = layer.self_attn\n",
    "\n",
    "        # access parameters of Conv1D\n",
    "        Wq = self_attention.q_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "        Wk = self_attention.k_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "\n",
    "        WqList.append(Wq)\n",
    "        WkList.append(Wk)\n",
    "\n",
    "    WqList = np.stack(WqList,axis=0)\n",
    "    WkList = np.stack(WkList,axis=0)\n",
    "\n",
    "    return WqList, WkList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52607685 0.53095376 0.50443894 ... 0.5699742  0.503946   0.51258475]\n",
      " [0.5351433  0.85908824 0.51482266 ... 0.51724714 0.5058044  0.5098453 ]\n",
      " [0.5556293  0.5044855  0.54950416 ... 0.66939104 0.5937965  0.5016917 ]\n",
      " ...\n",
      " [0.5050277  0.5271805  0.50715715 ... 0.489089   0.5075681  0.53097606]\n",
      " [0.50809693 0.5172196  0.47805548 ... 0.56132686 0.55978334 0.5196364 ]\n",
      " [0.505103   0.5139302  0.5019172  ... 0.55089754 0.5231217  0.48070505]]\n"
     ]
    }
   ],
   "source": [
    "layers = model.model.layers\n",
    "\n",
    "print(scores_trace_full(layers))\n",
    "%timeit scores_trace_full(layers)\n",
    "\n",
    "print(scores_trace_loop(h,l,layers))\n",
    "%timeit scores_trace_loop(h,l,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
