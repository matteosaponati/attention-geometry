{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.funs import scores, scores_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### numerical evaluation for the time complexity of computing the symmetry score\n",
    "## -----------\n",
    "### - d:   embedding dimension\n",
    "### - d_h: head dimension\n",
    "\n",
    "## method with numpy.linalg.norm\n",
    "## ----------\n",
    "## - compute the M matrix explicitly -> O(d^2 d_h)\n",
    "## - compute M + M.T \n",
    "## - compute np.linalg.norm for M and .5 * (M + M.T)\n",
    "## - compute the ratio of the two norms\n",
    "def scores_norm(Wq, Wk):\n",
    "    \n",
    "    M = Wq @ Wk.T\n",
    "    r = np.linalg.norm(.5 * (M + M.T), 'fro')**2 / np.linalg.norm(M, 'fro')**2\n",
    "\n",
    "    return r\n",
    "\n",
    "## method with np.einsum (leverage properties of M and cyclicity of trace)\n",
    "## ----------\n",
    "## - compute A, B, and C (without computing M explicitly)  -> O(d d_h^2)\n",
    "## - compute np.einsum of C with itself (fast way to compute the trace)\n",
    "## - compute np.einsum of A and B (fast way to compute the trace)\n",
    "## - compute the ratio of the two norms\n",
    "def scores_trace(Wq, Wk):\n",
    "     \n",
    "    ## (k x n) @ (n x k) -> O(nk^2)\n",
    "    A = Wq.T @ Wq\n",
    "    B = Wk.T @ Wk\n",
    "    C = Wk.T @ Wq\n",
    "\n",
    "    r = .5 * (1 + (np.einsum('ij,ji->', C, C) / np.einsum('ij,ji->', A, B)))\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test with BERT large\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "dh = 64\n",
    "d = 1024\n",
    "h = d // dh\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "Wq = model.encoder.layer[0].attention.self.query._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "Wk = model.encoder.layer[0].attention.self.key._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5953140874939474\n",
      "0.5953149540985507\n",
      "4.26 ms ± 328 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "634 µs ± 32.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    \n",
    "    print(scores_norm(Wq[:,j,:], Wk[:,j,:]))\n",
    "    print(scores_trace(Wq[:,j,:], Wk[:,j,:]))\n",
    "\n",
    "    %timeit scores_norm(Wq[:,j,:], Wk[:,j,:])\n",
    "    %timeit scores_trace(Wq[:,j,:], Wk[:,j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## test with LLAMA 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "'LLAMA 2 7b (l = 32, d = 4096, h = 32 ; tot num parameters 7B)'\n",
    "dh = 128\n",
    "l = 32\n",
    "d = 4096\n",
    "h = 32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "Wq = model.model.layers[0].self_attn.q_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "Wk = model.model.layers[0].self_attn.k_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5261237925015685\n",
      "0.5260768656918057\n",
      "123 ms ± 5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "4.94 ms ± 622 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    \n",
    "    print(scores_norm(Wq[:,j,:], Wk[:,j,:]))\n",
    "    print(scores_trace(Wq[:,j,:], Wk[:,j,:]))\n",
    "\n",
    "    %timeit scores_norm(Wq[:,j,:], Wk[:,j,:])\n",
    "    %timeit scores_trace(Wq[:,j,:], Wk[:,j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test with TInyGPT 8M parameters\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "'TinyGPT 8m (l = 8, d = 64, h = 4 ; 8M parameters)'\n",
    "dh = 64\n",
    "l = 8\n",
    "d = 256\n",
    "h = d // dh\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-8M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5100861340260628\n",
      "0.5100859254598618\n",
      "289 µs ± 26.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "205 µs ± 21.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5002862416666248\n",
      "0.5002863103582058\n",
      "325 µs ± 29.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "203 µs ± 12.7 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "0.5011066375787423\n",
      "0.5011066528968513\n",
      "305 µs ± 24.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "197 µs ± 13.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.5099731264739339\n",
      "0.5099732391536236\n",
      "273 µs ± 27.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "187 µs ± 14.8 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "0.5171045660543462\n",
      "0.5171047709882259\n",
      "252 µs ± 13.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "layers = model.transformer.h\n",
    "\n",
    "for i, layer in enumerate(layers):\n",
    "\n",
    "    # access self-attention module within layer\n",
    "    self_attention = layer.attn.attention\n",
    "\n",
    "    # access parameters of Conv1D\n",
    "    Wq = self_attention.q_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "    Wk = self_attention.k_proj._parameters[\"weight\"].T.view(d,h,dh).detach().numpy()\n",
    "\n",
    "    for j in range(h):\n",
    "            \n",
    "        print(scores_norm(Wq[:,j,:], Wk[:,j,:]))\n",
    "        print(scores_trace(Wq[:,j,:], Wk[:,j,:]))\n",
    "\n",
    "        %timeit scores_norm(Wq[:,j,:], Wk[:,j,:])\n",
    "        %timeit scores_trace(Wq[:,j,:], Wk[:,j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
