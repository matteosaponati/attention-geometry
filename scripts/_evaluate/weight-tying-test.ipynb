{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERT\n",
    "## models with bias terms\n",
    "\n",
    "from transformers import AutoModel, BertModel, DistilBertModel\n",
    "\n",
    "'BERT tiny (l = 2, d = 128, h = 2 ; 4.40M parameters)'\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "'BERT mini (l = 4, d = 256, h = 4 ; 11.3M parameters)'\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n",
    "\n",
    "'BERT small (l = 4, d = 512, h = 8 ; 29.1M parameters)'\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-512_A-8\")\n",
    "\n",
    "'BERT medium (l = 8, d = 512, h = 8 ; 41.7M parameters)'\n",
    "model = AutoModel.from_pretrained(\"google/bert_uncased_L-8_H-512_A-8\")\n",
    "\n",
    "'BERT base (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "'BERT large (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "'BERT large (masking) (l = 24, d = 1024, h = 16 ; 340M parameters)'\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "\n",
    "'DistillBERT base model (l = 6, d = 768, h = 12 ; tot num parameters 66M)'\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## ROBERTA\n",
    "## models with bias terms\n",
    "\n",
    "from transformers import RobertaModel, AutoModelForMaskedLM\n",
    "\n",
    "'ROBERTA base (l = 24, d = 1024, h = 16 ; 125M parameters)'\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "'ROBERTA large (l = 24, d = 1024, h = 16 ; 355M parameters)'\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "\n",
    "'DistillROBERTA base (l = 6, d = 768, h = 12 ; 82.2M parameters)'\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "np.where(model.roberta.embeddings.word_embeddings.weight.detach().numpy() != model.lm_head.decoder.weight.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALBERT models\n",
    "## models with bias terms\n",
    "\n",
    "from transformers import AlbertModel\n",
    "\n",
    "'ALBERT base model (l = 12, d = 768, h = 12 ; tot num parameters 11M)'\n",
    "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "'ALBERT large model (l = 24, d = 1024, h = 16 ; tot num parameters 17M)'\n",
    "model = AlbertModel.from_pretrained(\"albert-large-v2\")\n",
    "\n",
    "'ALBERT xlarge model (l = 24, d = 2048, h = 16 ; tot num parameters 58M)'\n",
    "model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
    "\n",
    "'ALBERT xxlarge model (l = 12, d = 4096, h = 64 ; tot num parameters 223M)'\n",
    "model = AlbertModel.from_pretrained(\"albert-xxlarge-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT models\n",
    "## models with bias terms\n",
    "\n",
    "from transformers import OpenAIGPTModel, GPT2Model, AutoModelForCausalLM\n",
    "\n",
    "'GPT 1 (l = 12, d = 768, h = 12 ; 110M parameters)'\n",
    "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "'GPT2 (l = 12, d = 768, h = 12 ; 117M parameters)'\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "'GPT2 medium (l = 24, d = 1024, h = 16 ; 345M parameters)'\n",
    "model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "\n",
    "'GPT2 large (l = 36, d = 1280, h = 20 ; 774M parameters)'\n",
    "model = GPT2Model.from_pretrained('gpt2-large')\n",
    "\n",
    "'GPT2 xl (l = 48, d = 1600, h = 25 ; 1558M parameters)'\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "\n",
    "'DistillGPT2 base model (l = 12, d = 768, h = 12 ; tot num parameters 82M)'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "np.where(model.transformer.wte.weight.detach().numpy() != model.lm_head.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.92s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GEMMA models\n",
    "## models WITHOUT bias\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "'Gemma model 2b (l = 18, d = 2048, h = 8, h_kv = 1 ; tot num parameters 2B)'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "np.where(model.model.embed_tokens.weight.detach().numpy() != model.lm_head.weight.detach().numpy())\n",
    "\n",
    "'Gemma model 7b (l = 18, d = 2048, h = 8, h_kv = 1 ; tot num parameters 2B)'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
    "np.where(model.model.embed_tokens.weight.detach().numpy() != model.lm_head.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/attention-geometry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([    0,     0,     0, ..., 31999, 31999, 31999]),\n",
       " array([   0,    1,    2, ..., 4093, 4094, 4095]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MISTRAL models (NO weight tying)\n",
    "## models WITHOUT bias\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "'Mistral model 7b (l = 32, d = 4096, h = 32, h_kv = 8 ; tot num parameters 7B)'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "np.where(model.model.embed_tokens.weight.detach().numpy() != model.lm_head.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.92s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([    0,     0,     0, ..., 31999, 31999, 31999]),\n",
       " array([   0,    1,    2, ..., 4093, 4094, 4095]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LLAMA2 models (NO weight tying)\n",
    "## models WITHOUT bias\n",
    "\n",
    "'LLAMA 2 7b (l = 32, d = 4096, h = 32 ; tot num parameters 2B)'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "np.where(model.model.embed_tokens.weight.detach().numpy() != model.lm_head.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30423927"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.model.embed_tokens.weight.detach().numpy() - model.lm_head.weight.detach().numpy()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
