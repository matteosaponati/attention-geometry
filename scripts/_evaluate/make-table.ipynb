{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {}\n",
    "\n",
    "language_models = ['BERT', 'ALBERT', 'ROBERTA', 'T5Encoder', 'GPT', 'GPT-neo', 'TinyGPT', 'LLAMA2', 'T5Decoder',\n",
    "                   'BEiT', 'DeiT', 'ViT', 'MAEDecoder', 'DINO', 'iGPT', 'GIT',\n",
    "                   'HUBERT', 'SpeechT5Encoder', 'MusicGenEncoder', 'SpeechT5Decoder', 'MusicGenDecoder']\n",
    "\n",
    "for m in language_models:\n",
    "\n",
    "    ## get the dictionary\n",
    "    if os.path.isfile(f'../../_data/fig-symmetry-language-models/full-models/{m}-query-key.pkl'):\n",
    "        dir = f'../../_data/fig-symmetry-language-models/full-models/{m}-query-key.pkl'\n",
    "        with open(dir, 'rb') as file:\n",
    "            models = pickle.load(file)\n",
    "    elif os.path.isfile(f'../../_data/fig-symmetry-vision-models/full-models/{m}-query-key.pkl'):\n",
    "        dir = f'../../_data/fig-symmetry-vision-models/full-models/{m}-query-key.pkl'\n",
    "        with open(dir, 'rb') as file:\n",
    "            models = pickle.load(file)\n",
    "    elif os.path.isfile(f'../../_data/fig-symmetry-audio-models/full-models/{m}-query-key.pkl'):\n",
    "        dir = f'../../_data/fig-symmetry-audio-models/full-models/{m}-query-key.pkl'\n",
    "        with open(dir, 'rb') as file:\n",
    "            models = pickle.load(file)\n",
    "\n",
    "    scores = [models[key][-1].flatten() for key in list(models.keys())]\n",
    "\n",
    "    for idx, key in enumerate(models.keys()):\n",
    "\n",
    "        scores_norm = 2 * scores[idx] - 1\n",
    "\n",
    "        median = np.median(scores_norm)\n",
    "        q1_range = median - np.percentile(scores_norm, 25)\n",
    "        q2_range = np.percentile(scores_norm, 75) - median\n",
    "\n",
    "        table[key] = [median, q1_range, q2_range]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\begin{table}\n",
      "\\label{table:symmetry-score-models}\n",
      "  \\caption{Symmetry score for open source pretrained language models. All models are available on Huggingface \\citep{wolfHuggingFaceTransformersStateoftheart2020}.}\n",
      "  \u000bspace{5pt}\n",
      "  \\centering\n",
      " egin{tabular}{lcc|lcc}\n",
      "    \toprule\n",
      "    \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} & \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} \\ \n",
      "    \\midrule\n",
      "BERT-tiny & 0.77 & $\\pm$ [0.15, 0.15] & BEiTbase-16-384 & 0.39 & $\\pm$ [0.23, 0.07] \\\\ \n",
      "BERT-mini & 0.62 & $\\pm$ [0.03, 0.05] & BEiTlarge-16-384 & 0.26 & $\\pm$ [0.17, 0.13] \\\\ \n",
      "BERT-small & 0.69 & $\\pm$ [0.1, 0.08] & DeiTtiny-16-224 & 0.47 & $\\pm$ [0.14, 0.06] \\\\ \n",
      "BERT-medium & 0.6 & $\\pm$ [0.01, 0.02] & DeiTsmall-16-224 & 0.52 & $\\pm$ [0.17, 0.06] \\\\ \n",
      "BERT-base & 0.51 & $\\pm$ [0.09, 0.07] & DeiTbase-16-224 & 0.47 & $\\pm$ [0.13, 0.17] \\\\ \n",
      "BERT-large & 0.44 & $\\pm$ [0.03, 0.08] & DeiTbase-16-384 & 0.54 & $\\pm$ [0.14, 0.06] \\\\ \n",
      "distill-BERT & 0.43 & $\\pm$ [0.12, 0.16] & ViT-base-16-224-pretrain & 0.36 & $\\pm$ [0.14, 0.2] \\\\ \n",
      "ABERT-base & 0.72 & $\\pm$ [0.0, 0.0] & ViT-large-16-224-pretrain & 0.15 & $\\pm$ [0.14, 0.25] \\\\ \n",
      "ALBERT-large & 0.7 & $\\pm$ [0.0, 0.0] & ViT-large-32-224-pretrain & 0.24 & $\\pm$ [0.02, 0.15] \\\\ \n",
      "ALBERT-xlarge & 0.59 & $\\pm$ [0.0, 0.0] & ViT-large-14-224-pretrain & 0.17 & $\\pm$ [0.07, 0.16] \\\\ \n",
      "ALBERT-xxlarge & 0.46 & $\\pm$ [0.0, 0.0] & ViTbase-16-224 & 0.36 & $\\pm$ [0.14, 0.2] \\\\ \n",
      "ROBERTA-base & 0.49 & $\\pm$ [0.03, 0.06] & ViTbase-16-384 & 0.36 & $\\pm$ [0.14, 0.2] \\\\ \n",
      "ROBERTA-large & 0.47 & $\\pm$ [0.06, 0.05] & ViTbase-32-384 & 0.35 & $\\pm$ [0.16, 0.2] \\\\ \n",
      "distill-ROBERTA & 0.53 & $\\pm$ [0.02, 0.07] & ViTlarge-16-224 & 0.16 & $\\pm$ [0.14, 0.25] \\\\ \n",
      "T5-small & -0.03 & $\\pm$ [0.05, 0.01] & ViTlarge-16-384 & 0.16 & $\\pm$ [0.14, 0.25] \\\\ \n",
      "T5-small-v1.1 & 0.02 & $\\pm$ [0.05, 0.03] & ViTlarge-32-384 & 0.24 & $\\pm$ [0.02, 0.15] \\\\ \n",
      "T5-base & 0.01 & $\\pm$ [0.01, 0.04] & MAE-base & 0.48 & $\\pm$ [0.07, 0.02] \\\\ \n",
      "T5-base-v1.1 & 0.07 & $\\pm$ [0.03, 0.02] & MAE-large & 0.43 & $\\pm$ [0.07, 0.07] \\\\ \n",
      "T5-large & 0.08 & $\\pm$ [0.01, 0.01] & MAE-huge & 0.46 & $\\pm$ [0.05, 0.03] \\\\ \n",
      "T5-large-v1.1 & 0.08 & $\\pm$ [0.05, 0.05] & DINO-small-8 & 0.3 & $\\pm$ [0.08, 0.2] \\\\ \n",
      "T5-3b & 0.23 & $\\pm$ [0.02, 0.01] & DINO-base-8 & 0.3 & $\\pm$ [0.16, 0.15] \\\\ \n",
      "T5-xl-v1.1 & 0.09 & $\\pm$ [0.02, 0.05] & DINO-small-16 & 0.33 & $\\pm$ [0.1, 0.1] \\\\ \n",
      "gpt1 & 0.07 & $\\pm$ [0.04, 0.03] & DINO-base-16 & 0.23 & $\\pm$ [0.11, 0.25] \\\\ \n",
      "gpt2 & 0.15 & $\\pm$ [0.02, 0.03] & DINOv2-small & 0.49 & $\\pm$ [0.02, 0.02] \\\\ \n",
      "gpt2-medium & 0.17 & $\\pm$ [0.03, 0.05] & DINOv2-base & 0.49 & $\\pm$ [0.03, 0.03] \\\\ \n",
      "gpt2-large & 0.17 & $\\pm$ [0.04, 0.02] & DINOv2-large & 0.45 & $\\pm$ [0.06, 0.04] \\\\ \n",
      "gpt2-xl & 0.12 & $\\pm$ [0.03, 0.05] & DINOv2-giant & 0.48 & $\\pm$ [0.08, 0.07] \\\\ \n",
      "distill-gpt2 & 0.19 & $\\pm$ [0.06, 0.06] & iGPT-small & 0.18 & $\\pm$ [0.03, 0.06] \\\\ \n",
      "gpt-neo-125m & 0.14 & $\\pm$ [0.09, 0.14] & iGPT-medium & 0.11 & $\\pm$ [0.02, 0.06] \\\\ \n",
      "gpt-neo-1.3b & 0.14 & $\\pm$ [0.03, 0.03] & iGPT-large & 0.14 & $\\pm$ [0.04, 0.13] \\\\ \n",
      "gpt-neo-2.7b & 0.13 & $\\pm$ [0.02, 0.04] & HUBERT-base & 0.3 & $\\pm$ [0.08, 0.07] \\\\ \n",
      "gpt-neo-6b & 0.11 & $\\pm$ [0.02, 0.03] & HUBERT-large & 0.43 & $\\pm$ [0.05, 0.05] \\\\ \n",
      "TinyGPT-1m & 0.02 & $\\pm$ [0.03, 0.02] & HUBERT-xlarge & 0.25 & $\\pm$ [0.03, 0.06] \\\\ \n",
      "TinyGPT-3m & 0.04 & $\\pm$ [0.03, 0.01] & speechT5-tts & 0.04 & $\\pm$ [0.01, 0.02] \\\\ \n",
      "TinyGPT-8m & -0.03 & $\\pm$ [0.12, 0.06] & speechT5-asr & 0.0 & $\\pm$ [0.02, 0.01] \\\\ \n",
      "TinyGPT-21m & 0.3 & $\\pm$ [0.0, 0.0] & speechT5-vc & 0.0 & $\\pm$ [0.01, 0.02] \\\\ \n",
      "TinyGPT-28m & 0.05 & $\\pm$ [0.05, 0.08] & MusicGen-small & 0.0 & $\\pm$ [0.0, 0.0] \\\\ \n",
      "LLAMA-7b & 0.12 & $\\pm$ [0.02, 0.03] & MusicGenStereo-small & 0.0 & $\\pm$ [0.0, 0.0] \\\\ \n",
      "BEiTbase-16-224-pt22k & 0.4 & $\\pm$ [0.08, 0.02] & MusicGen-medium & 0.0 & $\\pm$ [0.0, 0.0] \\\\ \n",
      "BEiTlarge-16-224-pt22k & 0.33 & $\\pm$ [0.05, 0.07] & MusicGenStereo-medium & 0.0 & $\\pm$ [0.0, 0.0] \\\\ \n",
      "BEiTbase-16-224 & 0.39 & $\\pm$ [0.23, 0.07] & MusicGen-large & 0.0 & $\\pm$ [0.0, 0.0] \\\\ \n",
      "BEiTlarge-16-224 & 0.26 & $\\pm$ [0.17, 0.13] & MusicGenStereo-large & 0.0 & $\\pm$ [0.0, 0.0] \\\\ \n",
      "    \\bottomrule\n",
      "  \\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# Extract keys and the last value of their associated lists\n",
    "medianList = [table[key][0] for key in list(table.keys())]\n",
    "q1List = [table[key][1] for key in list(table.keys())]\n",
    "q2List = [table[key][2] for key in list(table.keys())]\n",
    "\n",
    "# Determine the split point for two columns\n",
    "split_point = len(medianList) // 2\n",
    "\n",
    "# Create the LaTeX table\n",
    "latex_table = \"\"\"\\begin{table}\n",
    "\\label{table:symmetry-score-models}\n",
    "  \\caption{Symmetry score for open source pretrained language models. All models are available on Huggingface \\citep{wolfHuggingFaceTransformersStateoftheart2020}.}\n",
    "  \\vspace{5pt}\n",
    "  \\centering\n",
    "  \\begin{tabular}{lcc|lcc}\n",
    "    \\toprule\n",
    "    \\textbf{Model} & \\textbf{Median} & \\textbf{Interquartile range} & \\textbf{Model} & \\textbf{Median} & \\textbf{Interquartile range} \\\\ \n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Fill the first half\n",
    "for i in range(split_point):\n",
    "    name = list(table.keys())[i]\n",
    "    median = medianList[i]\n",
    "    q1 = q1List[i]\n",
    "    q2 = q2List[i]\n",
    "    corresponding_name = list(table.keys())[i + split_point] if i + split_point < len(table) else \"\"\n",
    "    corresponding_median = medianList[i + split_point] if i + split_point < len(table) else \"\"\n",
    "    corresponding_q1 = q1List[i + split_point] if i + split_point < len(table) else \"\"\n",
    "    corresponding_q2 = q2List[i + split_point] if i + split_point < len(table) else \"\"\n",
    "    latex_table += f\"{name} & {round(median, 2)} & $\\\\pm$ [{round(q1, 2)}, {round(q2, 2)}] & {corresponding_name} & {round(corresponding_median, 2)} & $\\\\pm$ [{round(corresponding_q1, 2)}, {round(corresponding_q2, 2)}] \\\\\\\\ \\n\"\n",
    "\n",
    "latex_table += \"\"\"    \\\\bottomrule\n",
    "  \\\\end{tabular}\n",
    "\\\\end{table}\"\"\"\n",
    "\n",
    "# Output the LaTeX table\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
